# vLLM Paged Attention å†…å­˜ç®¡ç†æ·±åº¦åˆ†æ

## æ ¸å¿ƒé—®é¢˜ï¼šå“ªäº›æ˜¯Pythonç®¡ç†ï¼Œå“ªäº›æ˜¯åº•å±‚å®ç°ï¼Ÿ

vLLMçš„paged attentionå†…å­˜ç®¡ç†æ˜¯ä¸€ä¸ª**å¤šå±‚æ··åˆæ¶æ„**ï¼Œ**ä¸æ˜¯å®Œå…¨é€šè¿‡Pythonç®¡ç†çš„**ã€‚è®©æˆ‘è¯¦ç»†ä¸ºæ‚¨åˆ†è§£å„ä¸ªå±‚æ¬¡ï¼š

## ğŸ—ï¸ æ¶æ„åˆ†å±‚

### 1. Pythonæ§åˆ¶å±‚ (é¡¶å±‚å†³ç­–)
**ä½ç½®**: `vllm/core/block_manager.py`, `vllm/core/block/cpu_gpu_block_allocator.py`

**èŒè´£**: 
- ğŸ“‹ **é€»è¾‘å†³ç­–**: ä½•æ—¶åˆ†é…ã€é‡Šæ”¾ã€äº¤æ¢block
- ğŸ§® **èµ„æºè§„åˆ’**: è®¡ç®—éœ€è¦å¤šå°‘blocksï¼Œwatermarkç­–ç•¥
- ğŸ”„ **è°ƒåº¦åè°ƒ**: ä¸scheduleré›†æˆï¼Œç®¡ç†block_tableæ˜ å°„

```python
# Pythonå±‚çš„å…¸å‹æ“ä½œ
class SelfAttnBlockSpaceManager:
    def allocate(self, seq_group: SequenceGroup) -> None:
        # Pythonè®¡ç®—éœ€è¦å¤šå°‘blocks
        num_required_blocks = BlockTable.get_num_required_blocks(...)
        # Pythonè°ƒç”¨åº•å±‚åˆ†é…å™¨
        block_table.allocate(token_ids=seq.get_token_ids())
    
    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:
        # Pythonå†³ç­–å“ªäº›blocksè¦swap
        seq_swap_mapping = self.block_allocator.swap(blocks=blocks,
                                                     src_device=Device.GPU,
                                                     dst_device=Device.CPU)
```

### 2. ä¸­é—´æŠ½è±¡å±‚ (æ¥å£æ¡¥æ¥)
**ä½ç½®**: `vllm/_custom_ops.py`, `vllm/attention/backends/`

**èŒè´£**:
- ğŸŒ‰ **æ¥å£ç»Ÿä¸€**: ä¸ºä¸åŒåç«¯ï¼ˆCUDA/ROCm/CPUï¼‰æä¾›ç»Ÿä¸€æ¥å£
- ğŸ¯ **å‚æ•°è½¬æ¢**: å°†Pythonå¯¹è±¡è½¬æ¢ä¸ºtensoræ“ä½œ
- ğŸ“Š **è®¾å¤‡ç®¡ç†**: å¤„ç†GPU/CPUè®¾å¤‡åˆ‡æ¢

```python
def swap_blocks(src: torch.Tensor, dst: torch.Tensor,
                block_mapping: torch.Tensor) -> None:
    # ç»Ÿä¸€æ¥å£ï¼Œåº•å±‚è·¯ç”±åˆ°å…·ä½“å®ç°
    torch.ops._C_cache_ops.swap_blocks(src, dst, block_mapping)
```

### 3. C++/CUDAå†…æ ¸å±‚ (åº•å±‚æ‰§è¡Œ)
**ä½ç½®**: `csrc/cache_kernels.cu`, `csrc/attention/attention_kernels.cuh`

**èŒè´£**:
- âš¡ **é«˜æ€§èƒ½æ‰§è¡Œ**: GPUå¹¶è¡Œå¤„ç†å®é™…çš„å†…å­˜æ“ä½œ
- ğŸ”— **ç¡¬ä»¶ä¼˜åŒ–**: åˆ©ç”¨CUDA shared memoryã€warpæ“ä½œ
- ğŸ“¦ **æ‰¹é‡å¤„ç†**: ä¸€æ¬¡å¤„ç†å¤šä¸ªblocksçš„æ‹·è´/äº¤æ¢

```cpp
// C++/CUDAå±‚çš„å®é™…æ‰§è¡Œ
void swap_blocks(torch::Tensor& src, torch::Tensor& dst,
                 const torch::Tensor& block_mapping) {
    // å®é™…çš„GPUå†…å­˜æ‹·è´æ“ä½œ
    for (size_t i = 0; i < num_blocks; i++) {
        cudaMemcpyAsync(dst_ptr + dst_offset, src_ptr + src_offset,
                        block_size_in_bytes, memcpy_type, stream);
    }
}
```

## ğŸ”„ å…·ä½“æ“ä½œçš„å®ç°å±‚æ¬¡

### Blockåˆ†é… (Allocation)
```mermaid
graph TD
    A[Python: Schedulerå†³ç­–] --> B[Python: BlockManager.allocate]
    B --> C[Python: BlockAllocatoråˆ†é…é€»è¾‘block]
    C --> D[Python: æ›´æ–°block_tableæ˜ å°„]
    D --> E[åº•å±‚: GPUå†…å­˜å·²é¢„åˆ†é…]
```

**ç‰¹ç‚¹**: ä¸»è¦åœ¨Pythonå±‚ï¼Œå› ä¸ºåˆ†é…æ˜¯é€»è¾‘æ“ä½œï¼ˆæ›´æ–°æ˜ å°„è¡¨ï¼‰ï¼Œç‰©ç†å†…å­˜å·²é¢„åˆ†é…ã€‚

### Blockäº¤æ¢ (Swapping)
```mermaid
graph TD
    A[Python: å†³ç­–swapç­–ç•¥] --> B[Python: ç”Ÿæˆblock_mapping]
    B --> C[C++: swap_blockså‡½æ•°]
    C --> D[CUDA: cudaMemcpyAsync]
    D --> E[ç¡¬ä»¶: GPUâ†”CPUæ•°æ®ä¼ è¾“]
```

**ç‰¹ç‚¹**: Pythonå†³ç­– + C++/CUDAæ‰§è¡Œï¼Œæ¶‰åŠçœŸå®çš„ç¡¬ä»¶å†…å­˜æ“ä½œã€‚

### Blockæ‹·è´ (Copy-on-Write)
```mermaid
graph TD
    A[Python: CoWè§¦å‘é€»è¾‘] --> B[Python: ç”Ÿæˆcopyæ˜ å°„]
    B --> C[CUDA: copy_blocks_kernel]
    C --> D[GPU: å¹¶è¡Œæ‹·è´å¤šä¸ªblocks]
```

**ç‰¹ç‚¹**: å®Œå…¨GPUå¹¶è¡ŒåŒ–ï¼ŒPythonåªè´Ÿè´£è§¦å‘ã€‚

### PagedAttentionè®¡ç®—
```mermaid
graph TD
    A[Python: æ„å»ºblock_tables] --> B[Python: è°ƒç”¨attention]
    B --> C[CUDA: paged_attention_v1/v2_kernel]
    C --> D[GPU: é«˜åº¦ä¼˜åŒ–çš„attentionè®¡ç®—]
    D --> E[ç¡¬ä»¶: Tensor CoreåŠ é€Ÿ]
```

**ç‰¹ç‚¹**: Pythonä¼ é€’å…ƒæ•°æ®ï¼Œæ ¸å¿ƒè®¡ç®—å®Œå…¨åœ¨GPUã€‚

## ğŸ’¾ å†…å­˜ç®¡ç†çš„å…·ä½“åˆ†å·¥

### Pythonè´Ÿè´£çš„éƒ¨åˆ†
1. **é€»è¾‘block IDç®¡ç†**
   ```python
   # é€»è¾‘IDåˆ°ç‰©ç†IDçš„æ˜ å°„
   self.block_tables: Dict[SeqId, BlockTable] = {}
   ```

2. **èµ„æºé¢„ç®—å’Œç­–ç•¥**
   ```python
   # è®¡ç®—watermarkï¼Œå†³ç­–æ˜¯å¦å¯ä»¥åˆ†é…
   if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:
       return AllocStatus.OK
   ```

3. **è®¾å¤‡é—´è°ƒåº¦å†³ç­–**
   ```python
   # å†³å®šå“ªäº›blockséœ€è¦swap
   seq_swap_mapping = self.block_allocator.swap(blocks=blocks,
                                                src_device=Device.GPU,
                                                dst_device=Device.CPU)
   ```

### C++/CUDAè´Ÿè´£çš„éƒ¨åˆ†
1. **ç‰©ç†å†…å­˜æ“ä½œ**
   ```cpp
   // å®é™…çš„å†…å­˜æ‹·è´
   cudaMemcpyAsync(dst_ptr + dst_offset, src_ptr + src_offset,
                   block_size_in_bytes, memcpy_type, stream);
   ```

2. **é«˜æ€§èƒ½attentionè®¡ç®—**
   ```cpp
   // ä¼˜åŒ–çš„paged attention kernel
   __global__ void paged_attention_v1_kernel(
       scalar_t* out, const scalar_t* q, 
       const cache_t* k_cache, const cache_t* v_cache,
       const int* block_tables, ...);
   ```

3. **GPUå¹¶è¡Œå¤„ç†**
   ```cpp
   // å¤šblockå¹¶è¡Œæ‹·è´
   template <typename scalar_t>
   __global__ void copy_blocks_kernel(...) {
       // æ¯ä¸ªGPU threadå¤„ç†ä¸€éƒ¨åˆ†æ•°æ®
   }
   ```

## ğŸ¯ å…³é”®è®¾è®¡ç†å¿µ

### 1. **é¢„åˆ†é…ç­–ç•¥**
- GPU/CPUå†…å­˜åœ¨åˆå§‹åŒ–æ—¶å°±åˆ†é…å¥½å›ºå®šå¤§å°çš„blockæ± 
- Pythonåªç®¡ç†é€»è¾‘IDï¼Œé¿å…é¢‘ç¹çš„malloc/free

### 2. **å¼‚æ­¥æ‰§è¡Œ**
- Pythonå¿«é€Ÿè¿”å›ï¼Œå®é™…å†…å­˜æ“ä½œåœ¨GPU streamä¸­å¼‚æ­¥æ‰§è¡Œ
- åˆ©ç”¨CUDA streamå¹¶è¡ŒåŒ–å¤šä¸ªæ“ä½œ

### 3. **é›¶æ‹·è´ä¼˜åŒ–**
- block_tableåªå­˜å‚¨æŒ‡é’ˆï¼Œä¸æ‹·è´å®é™…æ•°æ®
- Copy-on-Writeæœºåˆ¶å‡å°‘ä¸å¿…è¦çš„å†…å­˜æ‹·è´

### 4. **è®¾å¤‡æ„ŸçŸ¥**
- ä¸åŒè®¾å¤‡(GPU/CPU)æœ‰ä¸“é—¨çš„allocator
- è·¨è®¾å¤‡æ“ä½œé€šè¿‡ä¸“é—¨çš„swapæœºåˆ¶

## ğŸ“Š æ€§èƒ½åˆ†æ

| æ“ä½œç±»å‹ | Pythonå¼€é”€ | C++/CUDAå¼€é”€ | ä¸»è¦ç“¶é¢ˆ |
|---------|-----------|-------------|---------|
| Blockåˆ†é… | é«˜(æ˜ å°„è¡¨æ“ä½œ) | ä½ | Pythoné€»è¾‘ |
| Blocké‡Šæ”¾ | é«˜(æ¸…ç†æ˜ å°„) | ä½ | Pythoné€»è¾‘ |
| Blockäº¤æ¢ | ä½(ç”Ÿæˆæ˜ å°„) | é«˜ | PCIeå¸¦å®½ |
| Attentionè®¡ç®— | æä½ | é«˜ | GPUè®¡ç®— |
| CoWæ‹·è´ | æä½ | ä¸­ | GPUå†…å­˜å¸¦å®½ |

## ğŸ”— å®Œæ•´æºç é“¾è·¯å±•ç¤ºï¼šBlock Swapæ“ä½œ

ä¸ºäº†è®©æ‚¨æ›´æ·±å…¥ç†è§£vLLMçš„åˆ†å±‚æ¶æ„ï¼Œè®©æˆ‘ä»¥**Block Swapæ“ä½œ**ä¸ºä¾‹ï¼Œå±•ç¤ºä»Pythonå†³ç­–åˆ°CUDAæ‰§è¡Œçš„å®Œæ•´å®ç°é“¾è·¯ï¼š

### ç¬¬1å±‚ï¼šPythonè°ƒåº¦å±‚ - å†³ç­–è§¦å‘

**æ–‡ä»¶**: `vllm/core/scheduler.py`

```python
def _swap_out(
    self,
    seq_group: SequenceGroup,
    blocks_to_swap_out: List[Tuple[int, int]],
) -> None:
    """å°†sequence groupä»GPUæ¢å‡ºåˆ°CPU"""
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥swap outï¼ˆCPUå†…å­˜æ˜¯å¦è¶³å¤Ÿï¼‰
    if not self.block_manager.can_swap_out(seq_group):
        # æ— æ³•swapï¼Œéœ€è¦recompute
        self._preempt_by_recompute(seq_group)
        return
    
    # è§¦å‘å®é™…çš„swapæ“ä½œï¼Œè·å–blockæ˜ å°„
    mapping = self.block_manager.swap_out(seq_group)
    blocks_to_swap_out.extend(mapping)  # è®°å½•éœ€è¦swapçš„blockå¯¹
    
    # æ›´æ–°sequenceçŠ¶æ€
    for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
        seq.status = SequenceStatus.SWAPPED
        self._add_seq_group_to_swapped(seq_group)
```

**å…³é”®ä½œç”¨**: 
- ğŸ§  **å†³ç­–æ˜¯å¦swap**: æ£€æŸ¥CPUå†…å­˜å®¹é‡
- ğŸ“‹ **è§¦å‘swapæ“ä½œ**: è°ƒç”¨block_manager.swap_out()
- ğŸ”„ **çŠ¶æ€ç®¡ç†**: æ›´æ–°sequenceçŠ¶æ€ä¸ºSWAPPED

---

### ç¬¬2å±‚ï¼šPython Blockç®¡ç†å±‚ - é€»è¾‘æ˜ å°„

**æ–‡ä»¶**: `vllm/core/block_manager.py`

```python
def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:
    """å°†sequence groupçš„blocksä»GPUæ¢å‡ºåˆ°CPU
    
    Returns:
        List[Tuple[int, int]]: GPU block id -> CPU block idçš„æ˜ å°„åˆ—è¡¨
    """
    physical_block_id_mapping = []
    
    # éå†è¯¥sequence groupä¸­æ‰€æœ‰RUNNINGçŠ¶æ€çš„sequence
    for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
        blocks = self.block_tables[seq.seq_id].blocks
        if len(blocks) == 0:
            continue

        # è°ƒç”¨åº•å±‚allocatoræ‰§è¡Œå®é™…çš„swapæ“ä½œ
        seq_swap_mapping = self.block_allocator.swap(
            blocks=blocks,
            src_device=Device.GPU,
            dst_device=Device.CPU
        )

        # æ›´æ–°block tableä¸­çš„block idï¼ˆä»GPU idæ›´æ–°ä¸ºCPU idï¼‰
        self.block_tables[seq.seq_id].update(blocks)

        # å°†é€»è¾‘block idè½¬æ¢ä¸ºç‰©ç†block id
        seq_physical_block_id_mapping = {
            self.block_allocator.get_physical_block_id(Device.GPU, gpu_block_id):
            self.block_allocator.get_physical_block_id(Device.CPU, cpu_block_id)
            for gpu_block_id, cpu_block_id in seq_swap_mapping.items()
        }

        physical_block_id_mapping.extend(
            list(seq_physical_block_id_mapping.items())
        )

    return physical_block_id_mapping
```

**å…³é”®ä½œç”¨**:
- ğŸ—ºï¸ **é€»è¾‘æ˜ å°„**: ç®¡ç†sequence -> blocksçš„æ˜ å°„å…³ç³»
- ğŸ”„ **è®¾å¤‡è½¬æ¢**: åè°ƒGPUå’ŒCPUä¹‹é—´çš„blockè½¬ç§»
- ğŸ“Š **IDè½¬æ¢**: å°†é€»è¾‘block IDè½¬æ¢ä¸ºç‰©ç†block ID

---

### ç¬¬3å±‚ï¼šPythonåˆ†é…å™¨å±‚ - è®¾å¤‡åè°ƒ

**æ–‡ä»¶**: `vllm/core/block/cpu_gpu_block_allocator.py`

```python
def swap(self, blocks: List[Block], src_device: Device,
         dst_device: Device) -> Dict[int, int]:
    """åœ¨è®¾å¤‡é—´æ‰§è¡Œblock swapæ“ä½œ
    
    Args:
        blocks: è¦swapçš„blockåˆ—è¡¨
        src_device: æºè®¾å¤‡ (Device.GPU)
        dst_device: ç›®æ ‡è®¾å¤‡ (Device.CPU)
    
    Returns:
        Dict[int, int]: æºblock id -> ç›®æ ‡block idçš„æ˜ å°„
    """
    # è®°å½•æºè®¾å¤‡ä¸­çš„block IDs
    src_block_ids = [block.block_id for block in blocks]
    
    # åœ¨æºè®¾å¤‡ä¸Šé‡Šæ”¾è¿™äº›blocks
    self._allocators[src_device].swap_out(blocks)
    
    # åœ¨ç›®æ ‡è®¾å¤‡ä¸Šåˆ†é…å¯¹åº”çš„blocks
    self._allocators[dst_device].swap_in(blocks)
    
    # è·å–åœ¨ç›®æ ‡è®¾å¤‡ä¸Šæ–°åˆ†é…çš„block IDs  
    dst_block_ids = [block.block_id for block in blocks]

    # å»ºç«‹swapæ˜ å°„å¹¶è®°å½•åˆ°å…¨å±€swap_mappingä¸­
    current_swap_mapping: Dict[int, int] = {}
    for src_block_id, dst_block_id in zip(src_block_ids, dst_block_ids):
        if src_block_id is not None and dst_block_id is not None:
            self._swap_mapping[src_block_id] = dst_block_id
            current_swap_mapping[src_block_id] = dst_block_id
            
    return current_swap_mapping
```

**å…³é”®ä½œç”¨**:
- ğŸ›ï¸ **è®¾å¤‡åè°ƒ**: åè°ƒä¸åŒè®¾å¤‡çš„allocator
- ğŸ“ **æ˜ å°„è®°å½•**: ç»´æŠ¤å…¨å±€çš„swapæ˜ å°„å…³ç³»
- âš¡ **é«˜æ•ˆè½¬ç§»**: é¿å…å®é™…æ•°æ®æ‹·è´ï¼Œåªæ›´æ–°é€»è¾‘æ˜ å°„

---

### ç¬¬4å±‚ï¼šPython Workerå±‚ - æ‰§è¡Œè°ƒåº¦

**æ–‡ä»¶**: `vllm/worker/worker.py`

```python
def execute_model(
    self, 
    execute_model_req: ExecuteModelRequest
) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:
    """æ‰§è¡Œæ¨¡å‹æ¨ç†ï¼ŒåŒ…æ‹¬å†…å­˜swapæ“ä½œ"""
    
    # å°†swapè¯·æ±‚è½¬æ¢ä¸ºGPU tensorï¼ˆä¸ºäº†é«˜æ•ˆä¼ è¾“åˆ°GPUï¼‰
    blocks_to_swap_out = torch.tensor(
        execute_model_req.blocks_to_swap_out,
        device="cpu", 
        dtype=torch.int64
    ).view(-1, 2)
    
    # æ„å»ºworkerè¾“å…¥
    worker_input = WorkerInput(
        num_seq_groups=len(execute_model_req.seq_group_metadata_list),
        blocks_to_swap_out=blocks_to_swap_out,
        # ... å…¶ä»–å‚æ•°
    )
    
    # æ‰§è¡Œå®é™…çš„swapæ“ä½œ
    if (worker_input.blocks_to_swap_out is not None 
        and worker_input.blocks_to_swap_out.numel() > 0):
        self.cache_engine[virtual_engine].swap_out(
            worker_input.blocks_to_swap_out
        )
    
    # æ‰§è¡Œæ¨¡å‹æ¨ç†
    output = self.model_runner.execute_model(...)
    return output
```

**å…³é”®ä½œç”¨**:
- ğŸ“¦ **æ•°æ®æ‰“åŒ…**: å°†Python listè½¬æ¢ä¸ºGPU tensor
- ğŸš€ **æ‰§è¡Œè°ƒåº¦**: è°ƒç”¨cache_engineæ‰§è¡Œå®é™…swap
- ğŸ”— **æµç¨‹é›†æˆ**: å°†swapæ“ä½œé›†æˆåˆ°æ¨¡å‹æ‰§è¡Œæµç¨‹ä¸­

---

### ç¬¬5å±‚ï¼šPythonç¼“å­˜å¼•æ“å±‚ - å¤šå±‚å¤„ç†

**æ–‡ä»¶**: `vllm/worker/cache_engine.py`

```python
def swap_out(self, src_to_dst: torch.Tensor) -> None:
    """å¯¹æ‰€æœ‰attention layersæ‰§è¡Œswap outæ“ä½œ
    
    Args:
        src_to_dst: shapeä¸º(num_pairs, 2)çš„tensorï¼Œ
                   æ¯è¡ŒåŒ…å«[source_block_id, dest_block_id]
    """
    # éå†æ‰€æœ‰attentionå±‚ï¼Œæ¯å±‚éƒ½æœ‰ç‹¬ç«‹çš„KV cache
    for i in range(self.num_attention_layers):
        # è°ƒç”¨attention backendçš„swap_blocksæ–¹æ³•
        # self.gpu_cache[i] å’Œ self.cpu_cache[i] åˆ†åˆ«æ˜¯ç¬¬iå±‚çš„GPUå’ŒCPUç¼“å­˜
        self.attn_backend.swap_blocks(
            self.gpu_cache[i],  # æºï¼šGPUç¼“å­˜ (tuple: key_cache, value_cache)
            self.cpu_cache[i],  # ç›®æ ‡ï¼šCPUç¼“å­˜ (tuple: key_cache, value_cache) 
            src_to_dst         # blockæ˜ å°„å…³ç³»
        )
```

**å…³é”®ä½œç”¨**:
- ğŸ”„ **å¤šå±‚å¤„ç†**: å¯¹transformerçš„æ¯ä¸ªattentionå±‚éƒ½æ‰§è¡Œswap
- ğŸ—ï¸ **ç¼“å­˜ç®¡ç†**: ç®¡ç†æ¯å±‚çš„KV cacheç»“æ„
- ğŸ¯ **åç«¯è°ƒç”¨**: å°†æ“ä½œå§”æ‰˜ç»™å…·ä½“çš„attention backend

---

### ç¬¬6å±‚ï¼šPythonæ“ä½œæ¥å£å±‚ - ç»Ÿä¸€æŠ½è±¡

**æ–‡ä»¶**: `vllm/_custom_ops.py`

```python
def swap_blocks(src: torch.Tensor, dst: torch.Tensor,
                block_mapping: torch.Tensor) -> None:
    """ç»Ÿä¸€çš„block swapæ¥å£ï¼Œè·¯ç”±åˆ°å¯¹åº”çš„åº•å±‚å®ç°
    
    Args:
        src: æºtensorï¼ˆGPUä¸Šçš„KV cacheï¼‰
        dst: ç›®æ ‡tensorï¼ˆCPUä¸Šçš„KV cacheï¼‰
        block_mapping: shapeä¸º(num_pairs, 2)çš„æ˜ å°„tensor
    """
    # é€šè¿‡PyTorchçš„operator registryç³»ç»Ÿè°ƒç”¨åº•å±‚C++å®ç°
    # æ ¹æ®srcå’Œdstçš„deviceç±»å‹ï¼Œè‡ªåŠ¨è·¯ç”±åˆ°CUDAæˆ–CPUå®ç°
    torch.ops._C_cache_ops.swap_blocks(src, dst, block_mapping)
```

**å…³é”®ä½œç”¨**:
- ğŸŒ‰ **ç»Ÿä¸€æ¥å£**: æä¾›è®¾å¤‡æ— å…³çš„ç»Ÿä¸€API
- ğŸ¯ **è‡ªåŠ¨è·¯ç”±**: æ ¹æ®tensor deviceè‡ªåŠ¨é€‰æ‹©å®ç°
- ğŸ”§ **PyTorché›†æˆ**: é€šè¿‡operator registryä¸C++å±‚äº¤äº’

---

### ç¬¬7å±‚ï¼šC++ Bindingå±‚ - æ¥å£æ³¨å†Œ

**æ–‡ä»¶**: `csrc/torch_bindings.cpp`

```cpp
TORCH_LIBRARY_EXPAND(CONCAT(TORCH_EXTENSION_NAME, _cache_ops), cache_ops) {
    // æ³¨å†Œswap_blocksæ“ä½œåˆ°PyTorch operator registry
    cache_ops.def(
        "swap_blocks(Tensor src, Tensor! dst, Tensor block_mapping) -> ()"
    );
    
    // ä¸ºCUDAè®¾å¤‡æ³¨å†Œå…·ä½“å®ç°
    cache_ops.impl("swap_blocks", torch::kCUDA, &swap_blocks);
    
    // æ³¨æ„ï¼šCPUç‰ˆæœ¬çš„swap_blocksä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºCPUä¸æ”¯æŒæ­¤æ“ä½œ
}
```

**å…³é”®ä½œç”¨**:
- ğŸ“‹ **æ¥å£æ³¨å†Œ**: å‘PyTorchæ³¨å†ŒC++å‡½æ•°
- ğŸ¯ **è®¾å¤‡åˆ†å‘**: ä¸ºä¸åŒè®¾å¤‡ç±»å‹æ³¨å†Œä¸åŒå®ç°
- ğŸ”— **Python-C++æ¡¥æ¥**: è¿æ¥Pythonè°ƒç”¨å’ŒC++å®ç°

---

### ç¬¬8å±‚ï¼šCUDAå®ç°å±‚ - ç¡¬ä»¶æ‰§è¡Œ

**æ–‡ä»¶**: `csrc/cache_kernels.cu`

```cpp
void swap_blocks(torch::Tensor& src, torch::Tensor& dst,
                 const torch::Tensor& block_mapping) {
    // ç¡®å®šå†…å­˜æ‹·è´ç±»å‹ï¼ˆGPU->CPU, CPU->GPU, or GPU->GPUï¼‰
    torch::Device src_device = src.device();
    torch::Device dst_device = dst.device();
    cudaMemcpyKind memcpy_type;
    
    if (src_device.is_cuda() && dst_device.is_cpu()) {
        memcpy_type = cudaMemcpyDeviceToHost;  // GPU -> CPU
    } else if (src_device.is_cpu() && dst_device.is_cuda()) {
        memcpy_type = cudaMemcpyHostToDevice;  // CPU -> GPU  
    } else if (src_device.is_cuda() && dst_device.is_cuda()) {
        memcpy_type = cudaMemcpyDeviceToDevice; // GPU -> GPU
    } else {
        TORCH_CHECK(false, "Invalid device combination");
    }

    // è·å–åŸå§‹å†…å­˜æŒ‡é’ˆ
    char* src_ptr = static_cast<char*>(src.data_ptr());
    char* dst_ptr = static_cast<char*>(dst.data_ptr());

    // è®¡ç®—æ¯ä¸ªblockçš„å­—èŠ‚å¤§å°ï¼ˆåŒ…æ‹¬paddingï¼‰
    const int64_t block_size_in_bytes = src.element_size() * src.stride(0);
    
    // è®¾ç½®CUDAè®¾å¤‡å’Œstream
    const at::cuda::OptionalCUDAGuard device_guard(
        src_device.is_cuda() ? src_device : dst_device
    );
    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
    
    // éå†æ‰€æœ‰éœ€è¦swapçš„block pairs
    const int64_t num_blocks = block_mapping.size(0);
    for (size_t i = 0; i < num_blocks; i++) {
        // è·å–æºå’Œç›®æ ‡blockç¼–å·
        int64_t src_block_number = block_mapping[i][0].item<int64_t>();
        int64_t dst_block_number = block_mapping[i][1].item<int64_t>();
        
        // è®¡ç®—å†…å­˜åç§»é‡
        int64_t src_offset = src_block_number * block_size_in_bytes;
        int64_t dst_offset = dst_block_number * block_size_in_bytes;
        
        // å¼‚æ­¥å†…å­˜æ‹·è´ï¼ˆå…³é”®çš„ç¡¬ä»¶æ“ä½œï¼ï¼‰
        cudaMemcpyAsync(
            dst_ptr + dst_offset,    // ç›®æ ‡åœ°å€
            src_ptr + src_offset,    // æºåœ°å€  
            block_size_in_bytes,     // æ‹·è´å­—èŠ‚æ•°
            memcpy_type,             // æ‹·è´ç±»å‹
            stream                   // CUDA streamï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
        );
    }
    // æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰cudaStreamSynchronize()ï¼Œæ‰€ä»¥æ˜¯å®Œå…¨å¼‚æ­¥çš„ï¼
}
```

**å…³é”®ä½œç”¨**:
- ğŸ”§ **ç¡¬ä»¶æ“ä½œ**: ç›´æ¥è°ƒç”¨CUDAè¿›è¡Œå†…å­˜æ‹·è´
- âš¡ **å¼‚æ­¥æ‰§è¡Œ**: ä½¿ç”¨cudaMemcpyAsyncé¿å…é˜»å¡
- ğŸ¯ **è®¾å¤‡æ£€æµ‹**: è‡ªåŠ¨å¤„ç†ä¸åŒè®¾å¤‡ç»„åˆ
- ğŸ“Š **æ‰¹é‡å¤„ç†**: ä¸€æ¬¡å¤„ç†å¤šä¸ªblockçš„æ‹·è´

---

## ğŸ” è°ƒç”¨é“¾è·¯æ€»ç»“

å®Œæ•´çš„è°ƒç”¨é“¾è·¯å¦‚ä¸‹ï¼š

```
1. Scheduler._swap_out()           [Pythonå†³ç­–å±‚]
      â†“
2. BlockManager.swap_out()         [Pythoné€»è¾‘å±‚] 
      â†“
3. CpuGpuBlockAllocator.swap()     [Pythonåˆ†é…å™¨å±‚]
      â†“
4. Worker.execute_model()          [Pythonæ‰§è¡Œå±‚]
      â†“  
5. CacheEngine.swap_out()          [Pythonç¼“å­˜å±‚]
      â†“
6. _custom_ops.swap_blocks()       [Pythonæ¥å£å±‚]
      â†“
7. torch.ops._C_cache_ops.swap_blocks()  [C++ Bindingå±‚]
      â†“
8. swap_blocks() in cache_kernels.cu     [CUDAæ‰§è¡Œå±‚]
      â†“
9. cudaMemcpyAsync()               [ç¡¬ä»¶å±‚]
```

## ğŸ¯ å…³é”®è®¾è®¡äº®ç‚¹

### 1. **åˆ†å±‚è§£è€¦**
- **ä¸Šå±‚**ï¼šPythonè´Ÿè´£é€»è¾‘å†³ç­–ã€èµ„æºç®¡ç†ã€çŠ¶æ€ç»´æŠ¤
- **ä¸‹å±‚**ï¼šC++/CUDAè´Ÿè´£é«˜æ€§èƒ½çš„å†…å­˜æ“ä½œå’Œè®¡ç®—

### 2. **å¼‚æ­¥æ‰§è¡Œ**
- **Pythonå±‚å¿«é€Ÿè¿”å›**ï¼šä¸ç­‰å¾…å®é™…å†…å­˜æ‹·è´å®Œæˆ
- **CUDAå¼‚æ­¥æ‹·è´**ï¼š`cudaMemcpyAsync`åœ¨GPU streamä¸­å¹¶è¡Œæ‰§è¡Œ
- **æµæ°´çº¿ä¼˜åŒ–**ï¼šå†…å­˜æ‹·è´ä¸æ¨¡å‹è®¡ç®—å¯ä»¥å¹¶è¡Œè¿›è¡Œ

### 3. **é›¶æ‹·è´è®¾è®¡**
- **é€»è¾‘swap**ï¼šPythonå±‚åªæ›´æ–°block IDæ˜ å°„ï¼Œä¸æ‹·è´æ•°æ®
- **ç‰©ç†swap**ï¼šåªåœ¨çœŸæ­£éœ€è¦æ—¶æ‰è°ƒç”¨CUDAè¿›è¡Œå®é™…å†…å­˜ä¼ è¾“
- **æ‰¹é‡ä¼˜åŒ–**ï¼šä¸€æ¬¡è°ƒç”¨å¤„ç†å¤šä¸ªblocksï¼Œå‡å°‘è°ƒç”¨å¼€é”€

### 4. **è®¾å¤‡æŠ½è±¡**
- **ç»Ÿä¸€æ¥å£**ï¼šåŒä¸€ä¸ª`swap_blocks()`é€‚ç”¨äºGPUâ†”CPUã€GPUâ†”GPU
- **è‡ªåŠ¨è·¯ç”±**ï¼šæ ¹æ®tensor deviceè‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–çš„å®ç°è·¯å¾„
- **é”™è¯¯å¤„ç†**ï¼šéæ³•è®¾å¤‡ç»„åˆä¼šè¢«åŠæ—¶æ£€æµ‹å’ŒæŠ¥é”™

## â° Blockæ“ä½œåœ¨Stepä¸­çš„å…·ä½“æ—¶æœº

æ‚¨é—®å¾—å¾ˆå¥½ï¼blockæ“ä½œç¡®å®æ˜¯ç”±**scheduler**åœ¨æ¯ä¸ªstepä¸­åè°ƒçš„ï¼Œä½†æœ‰ä¸¥æ ¼çš„æ‰§è¡Œæ—¶æœºã€‚è®©æˆ‘è¯¦ç»†è§£é‡Šï¼š

### Stepæ‰§è¡Œçš„ä¸‰å¤§é˜¶æ®µ

```mermaid
graph TB
    subgraph "Stepå¼€å§‹"
        A[æ£€æŸ¥æ˜¯å¦æœ‰å‰©ä½™çš„å¤šæ­¥éª¤ä»»åŠ¡]
    end
    
    subgraph "ğŸ¯ é˜¶æ®µ1: Schedulerè°ƒåº¦å†³ç­–"
        B[scheduler schedule] 
        C[_schedule_running: å¤„ç†RUNNINGé˜Ÿåˆ—]
        D[_schedule_swapped: å¤„ç†SWAPPEDé˜Ÿåˆ—] 
        E[_schedule_prefills: å¤„ç†WAITINGé˜Ÿåˆ—]
        F[ç”ŸæˆSchedulerOutputs]
        G[blocks_to_swap_in/out/copy]
    end
    
    subgraph "ğŸš€ é˜¶æ®µ2: Workeræ‰§è¡Œæ“ä½œ"
        H[Worker execute model]
        I[ğŸ”„ Block Swapæ“ä½œ]
        J[ğŸ“¦ Block Copyæ“ä½œ] 
        K[ğŸ§  æ¨¡å‹æ¨ç†]
        L[ğŸ“Š Attentionè®¡ç®—]
    end
    
    subgraph "ğŸ“¤ é˜¶æ®µ3: è¾“å‡ºå¤„ç†"
        M[å¤„ç†æ¨¡å‹è¾“å‡º]
        N[æ›´æ–°sequenceçŠ¶æ€]
        O[é‡Šæ”¾å®Œæˆçš„åºåˆ—]
        P[è¿”å›RequestOutput]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E 
    E --> F
    F --> G
    G --> H
    H --> I
    I --> J
    J --> K
    K --> L
    L --> M
    M --> N
    N --> O
    O --> P
    
    style B fill:#e1f5fe
    style I fill:#ffecb3  
    style J fill:#ffecb3
    style K fill:#c8e6c9
```

### ğŸ¯ é˜¶æ®µ1: Schedulerè°ƒåº¦å†³ç­– (Blockæ“ä½œçš„å†³ç­–é˜¶æ®µ)

**æ–‡ä»¶**: `vllm/core/scheduler.py`

```python
def schedule(self) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs, bool]:
    """è°ƒåº¦å™¨çš„æ ¸å¿ƒæ–¹æ³• - å†³å®šblockæ“ä½œç­–ç•¥"""
    
    budget = SchedulingBudget(
        token_budget=self.scheduler_config.max_num_batched_tokens,
        max_num_seqs=self.scheduler_config.max_num_seqs,
    )
    
    # 1ï¸âƒ£ ä¼˜å…ˆå¤„ç†RUNNINGé˜Ÿåˆ— (å¯èƒ½è§¦å‘swap_out)
    running_scheduled = self._schedule_running(budget, curr_loras, enable_chunking)
    # åœ¨è¿™é‡Œå†³å®šï¼šå“ªäº›åºåˆ—éœ€è¦è¢«preemptå’Œswap out
    
    # 2ï¸âƒ£ å°è¯•swap inä¸€äº›SWAPPEDåºåˆ—
    swapped_in = self._schedule_swapped(budget, curr_loras, enable_chunking)  
    # åœ¨è¿™é‡Œå†³å®šï¼šå“ªäº›åºåˆ—å¯ä»¥ä»CPU swapå›GPU
    
    # 3ï¸âƒ£ æœ€åè°ƒåº¦WAITINGé˜Ÿåˆ— (åˆ†é…æ–°blocks)
    prefills = self._schedule_prefills(budget, curr_loras, enable_chunking)
    # åœ¨è¿™é‡Œå†³å®šï¼šå“ªäº›æ–°è¯·æ±‚å¯ä»¥å¼€å§‹prefill
    
    # ğŸ”¥ å…³é”®ï¼šç”Ÿæˆblockæ“ä½œæŒ‡ä»¤
    scheduler_outputs = SchedulerOutputs(
        blocks_to_swap_in=swapped_in.blocks_to_swap_in,     # ğŸ“¥ CPUâ†’GPU
        blocks_to_swap_out=running_scheduled.blocks_to_swap_out,  # ğŸ“¤ GPUâ†’CPU  
        blocks_to_copy=running_scheduled.blocks_to_copy,    # ğŸ“‹ GPUå†…æ‹·è´
        # ... å…¶ä»–è¾“å‡º
    )
    
    return seq_group_metadata_list, scheduler_outputs, allow_async_output_proc
```

**â° æ—¶æœº**: **Stepå¼€å§‹æ—¶**ï¼Œscheduleråšå‡ºæ‰€æœ‰blockæ“ä½œçš„å†³ç­–
**ğŸ§  å†³ç­–å†…å®¹**:
- ğŸ“¤ **Swap Out**: å“ªäº›runningåºåˆ—éœ€è¦æ¢å‡ºåˆ°CPU (å†…å­˜ä¸è¶³æ—¶)
- ğŸ“¥ **Swap In**: å“ªäº›swappedåºåˆ—å¯ä»¥æ¢å›GPU (æœ‰ç©ºé—²å†…å­˜æ—¶) 
- ğŸ“‹ **Copy**: å“ªäº›blockséœ€è¦CoWæ‹·è´ (beam searchåˆ†å‰æ—¶)

---

### ğŸš€ é˜¶æ®µ2: Workeræ‰§è¡Œæ“ä½œ (Blockæ“ä½œçš„æ‰§è¡Œé˜¶æ®µ)

**æ–‡ä»¶**: `vllm/worker/worker.py`

```python
def execute_model(self, execute_model_req: ExecuteModelRequest):
    """Workeræ‰§è¡Œæ¨¡å‹ - å®é™…æ‰§è¡Œblockæ“ä½œ"""
    
    # ğŸ“¦ æ•°æ®å‡†å¤‡ï¼šå°†è°ƒåº¦å†³ç­–è½¬æ¢ä¸ºtensor
    blocks_to_swap_in = torch.tensor(execute_model_req.blocks_to_swap_in, 
                                     device="cpu", dtype=torch.int64)
    blocks_to_swap_out = torch.tensor(execute_model_req.blocks_to_swap_out,
                                      device="cpu", dtype=torch.int64) 
    blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,
                                  device="cpu", dtype=torch.int64)
    
    # ğŸ”„ ç¬¬ä¸€æ­¥ï¼šæ‰§è¡ŒBlockæ“ä½œ (åœ¨æ¨¡å‹æ¨ç†ä¹‹å‰ï¼)
    if blocks_to_swap_in.numel() > 0:
        self.cache_engine.swap_in(blocks_to_swap_in)    # ğŸ“¥ CPUâ†’GPU
        
    if blocks_to_swap_out.numel() > 0:  
        self.cache_engine.swap_out(blocks_to_swap_out)  # ğŸ“¤ GPUâ†’CPU
        
    if blocks_to_copy.numel() > 0:
        self.cache_engine.copy(blocks_to_copy)          # ğŸ“‹ GPUå†…æ‹·è´
    
    # ğŸ§  ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œæ¨¡å‹æ¨ç† (blockæ“ä½œå®Œæˆå)
    output = self.model_runner.execute_model(
        model_input=model_input,
        kv_caches=self.kv_cache,  # ä½¿ç”¨æ›´æ–°åçš„KV cache
        # ...
    )
    
    return output
```

**â° æ—¶æœº**: **æ¨¡å‹æ¨ç†ä¹‹å‰**ï¼Œå…ˆæ‰§è¡Œæ‰€æœ‰blockæ“ä½œ
**ğŸš€ æ‰§è¡Œé¡ºåº**:
1. **Swap In** â†’ **Swap Out** â†’ **Copy** â†’ **æ¨¡å‹æ¨ç†**
2. ç¡®ä¿KV cacheåœ¨æ¨¡å‹è®¡ç®—å‰å°±å‡†å¤‡å¥½æ­£ç¡®çš„æ•°æ®

---

### ğŸ“¤ é˜¶æ®µ3: è¾“å‡ºå¤„ç† (Blockæ“ä½œçš„æ¸…ç†é˜¶æ®µ)

**æ–‡ä»¶**: `vllm/engine/llm_engine.py`

```python
def step(self) -> List[RequestOutput]:
    """Engine stepçš„æœ€ç»ˆé˜¶æ®µ"""
    
    # ... å‰é¢çš„è°ƒåº¦å’Œæ‰§è¡Œ ...
    
    # ğŸ“Š å¤„ç†æ¨¡å‹è¾“å‡º
    request_outputs = self._process_model_outputs(
        outputs=outputs, 
        scheduler_outputs=scheduler_outputs,
        seq_group_metadata_list=seq_group_metadata_list
    )
    
    # ğŸ—‘ï¸ æ¸…ç†å®Œæˆçš„åºåˆ— (é‡Šæ”¾blocks)
    self.scheduler.free_finished_seq_groups()
    
    return request_outputs

def free_finished_seq_groups(self) -> None:
    """é‡Šæ”¾å·²å®Œæˆåºåˆ—çš„blocks"""
    for seq_group in self.running[:]:
        if seq_group.is_finished():
            # ğŸ”„ é‡Šæ”¾GPU blocks
            self._free_finished_seqs(seq_group) 
            self.running.remove(seq_group)
```

**â° æ—¶æœº**: **Stepç»“æŸæ—¶**ï¼Œæ¸…ç†å®Œæˆçš„åºåˆ—
**ğŸ—‘ï¸ æ¸…ç†æ“ä½œ**:
- é‡Šæ”¾å·²å®Œæˆåºåˆ—çš„GPU blocks
- æ›´æ–°free blockè®¡æ•°
- ä¸ºä¸‹ä¸€ä¸ªstepè…¾å‡ºç©ºé—´

---

## ğŸ¯ å…³é”®æ—¶æœºæ€»ç»“

### Blockæ“ä½œçš„ä¸¥æ ¼é¡ºåºï¼š

```
ğŸ“… Step Nå¼€å§‹:
â”œâ”€â”€ ğŸ¯ Schedulerå†³ç­– (0.1-1ms)
â”‚   â”œâ”€â”€ åˆ†æå†…å­˜çŠ¶å†µ
â”‚   â”œâ”€â”€ å†³å®šswap_in/swap_out/copy
â”‚   â””â”€â”€ ç”ŸæˆExecuteModelRequest
â”œâ”€â”€ ğŸš€ Workeræ‰§è¡Œ (10-100ms)  
â”‚   â”œâ”€â”€ ğŸ”„ Block Swapæ“ä½œ (1-10ms)
â”‚   â”œâ”€â”€ ğŸ“¦ Block Copyæ“ä½œ (0.1-1ms)  
â”‚   â””â”€â”€ ğŸ§  æ¨¡å‹æ¨ç† (10-90ms)
â””â”€â”€ ğŸ“¤ è¾“å‡ºå¤„ç† (0.1-1ms)
    â”œâ”€â”€ å¤„ç†ç»“æœ
    â””â”€â”€ ğŸ—‘ï¸ æ¸…ç†å®Œæˆçš„åºåˆ—

ğŸ“… Step N+1å¼€å§‹...
```

### ğŸ’¡ è®¾è®¡æ™ºæ…§

1. **ğŸ¯ å†³ç­–ä¸æ‰§è¡Œåˆ†ç¦»**: Scheduleråªåšå†³ç­–ï¼ŒWorkerè´Ÿè´£æ‰§è¡Œ
2. **â° ä¸¥æ ¼çš„æ—¶æœºæ§åˆ¶**: Blockæ“ä½œæ€»æ˜¯åœ¨æ¨¡å‹æ¨ç†ä¹‹å‰å®Œæˆ  
3. **ğŸ”„ å¼‚æ­¥ä¼˜åŒ–**: Block swapä½¿ç”¨CUDA streamï¼Œä¸åç»­è®¡ç®—å¹¶è¡Œ
4. **ğŸ“Š èµ„æºæ„ŸçŸ¥**: æ ¹æ®GPUå†…å­˜çŠ¶å†µåŠ¨æ€è°ƒæ•´blockæ“ä½œç­–ç•¥

æ‰€ä»¥å›ç­”æ‚¨çš„é—®é¢˜ï¼š**Blockæ“ä½œç¡®å®æ˜¯ç”±Scheduleråœ¨æ¯ä¸ªstepå¼€å§‹æ—¶å†³ç­–çš„ï¼Œä½†å®é™…æ‰§è¡Œæ˜¯åœ¨Workerå±‚ï¼Œä¸”æ€»æ˜¯åœ¨æ¨¡å‹æ¨ç†ä¹‹å‰å®Œæˆï¼**

## ğŸ¯ æ€»ç»“

**vLLMçš„paged attentionå†…å­˜ç®¡ç†ä¸æ˜¯å®Œå…¨Pythonç®¡ç†çš„**ï¼Œè€Œæ˜¯ï¼š

1. **Pythonå±‚**: è´Ÿè´£é€»è¾‘å†³ç­–ã€èµ„æºè§„åˆ’ã€è°ƒåº¦åè°ƒ
2. **C++å±‚**: æä¾›ç»Ÿä¸€æ¥å£ï¼Œå¤„ç†è®¾å¤‡æŠ½è±¡  
3. **CUDAå±‚**: æ‰§è¡Œé«˜æ€§èƒ½çš„å®é™…å†…å­˜æ“ä½œå’Œè®¡ç®—

è¿™ç§**åˆ†å±‚æ¶æ„**å……åˆ†å‘æŒ¥äº†å„å±‚çš„ä¼˜åŠ¿ï¼š
- Pythonçš„çµæ´»æ€§ç”¨äºå¤æ‚é€»è¾‘
- C++çš„æ€§èƒ½ç”¨äºç³»ç»Ÿè°ƒç”¨
- CUDAçš„å¹¶è¡Œæ€§ç”¨äºå¤§è§„æ¨¡è®¡ç®—

å…³é”®åœ¨äº**Pythonæ§åˆ¶ç­–ç•¥ï¼ŒCUDAæ‰§è¡Œæ“ä½œ**ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜ç®¡ç†å’Œè®¡ç®—æ€§èƒ½çš„å®Œç¾ç»“åˆã€‚ 


## å›¾
```mermaid
graph TB
    subgraph "Pythonæ§åˆ¶å±‚"
        A[Scheduler è°ƒåº¦å†³ç­–]
        B[BlockManager èµ„æºç®¡ç†]
        C[BlockAllocator é€»è¾‘åˆ†é…]
        D[BlockTable æ˜ å°„ç»´æŠ¤]
    end
    
    subgraph "æŠ½è±¡æ¥å£å±‚"
        E[_custom_ops ç»Ÿä¸€æ¥å£]
        F[AttentionBackend åç«¯æŠ½è±¡]
        G[CacheEngine ç¼“å­˜å¼•æ“]
    end
    
    subgraph "C++/CUDAæ‰§è¡Œå±‚"
        H[cache_kernels.cu ç¼“å­˜æ“ä½œ]
        I[attention_kernels.cu æ³¨æ„åŠ›è®¡ç®—]
        J[copy_blocks_kernel GPUå¹¶è¡Œæ‹·è´]
        K[swap_blocks cudaMemcpyAsync]
        L[paged_attention_kernel ä¼˜åŒ–è®¡ç®—]
    end
    
    subgraph "ç¡¬ä»¶å±‚"
        M[GPU Memory Pool]
        N[CPU Memory Pool]
        O[PCIe ä¼ è¾“]
        P[Tensor Core åŠ é€Ÿ]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    G --> I
    H --> J
    H --> K
    I --> L
    J --> M
    K --> O
    L --> P
    M --> N
    
    style A fill:#e1f5fe
    style H fill:#ffecb3
    style M fill:#c8e6c9
```

## ğŸ—ºï¸ Block Tableï¼šPaged Attentionçš„æ ¸å¿ƒå†…å­˜æ˜ å°„è¡¨

æ‚¨é—®å¾—éå¸¸å¥½ï¼**Block Table**ç¡®å®æ˜¯paged attentionçš„æ ¸å¿ƒï¼Œå®ƒå°±æ˜¯æ‚¨è¯´çš„"å†…å­˜table"ã€‚è®©æˆ‘è¯¦ç»†ä¸ºæ‚¨å±•ç¤ºè¿™ä¸ªtableåœ¨vLLMä¸­çš„å…·ä½“ä½“ç°å’Œå®Œæ•´å·¥ä½œæœºåˆ¶ã€‚

### ğŸ“ Block Tableåœ¨vLLMä¸­çš„å…·ä½“ä½ç½®

#### 1. **Pythonç®¡ç†å±‚ - é€»è¾‘Block Table**

**æ–‡ä»¶**: `vllm/core/block_manager.py`

```python
class SelfAttnBlockSpaceManager(BlockSpaceManager):
    def __init__(self, ...):
        # ğŸ—ºï¸ æ ¸å¿ƒï¼šæ¯ä¸ªsequenceéƒ½æœ‰è‡ªå·±çš„block table
        self.block_tables: Dict[int, BlockTable] = {}  # seq_id -> BlockTable
        
    def allocate(self, seq_group: SequenceGroup) -> None:
        """ä¸ºæ–°åºåˆ—åˆ†é…block table"""
        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]
        
        # ğŸ”¥ åˆ›å»ºæ–°çš„block table
        block_table: BlockTable = self._allocate_sequence(seq)
        self.block_tables[seq.seq_id] = block_table  # å­˜å‚¨æ˜ å°„
        
    def get_block_table(self, seq: Sequence) -> List[int]:
        """è·å–åºåˆ—çš„ç‰©ç†block IDåˆ—è¡¨"""
        # ğŸ¯ è¿”å›é€»è¾‘åˆ°ç‰©ç†çš„æ˜ å°„
        block_ids = self.block_tables[seq.seq_id].physical_block_ids
        return block_ids  # [ç‰©ç†block0, ç‰©ç†block1, ç‰©ç†block2, ...]
```

#### 2. **æ•°æ®ä¼ é€’å±‚ - SequenceGroupMetadata**

**æ–‡ä»¶**: `vllm/sequence.py`

```python
class SequenceGroupMetadata:
    """ä¼ é€’ç»™Workerçš„å…ƒæ•°æ®ï¼ŒåŒ…å«block table"""
    
    # ğŸ—ºï¸ æ ¸å¿ƒï¼šblock tableså­—æ®µ
    block_tables: dict[int, list[int]]  # seq_id -> [physical_block_ids]
    
    # ç¤ºä¾‹ï¼š
    # block_tables = {
    #     seq_id_1: [10, 15, 23, 8],   # åºåˆ—1çš„ç‰©ç†blocks
    #     seq_id_2: [2, 7, 19],        # åºåˆ—2çš„ç‰©ç†blocks  
    #     seq_id_3: [31, 12, 4, 18]    # åºåˆ—3çš„ç‰©ç†blocks
    # }
```

#### 3. **Workerå¤„ç†å±‚ - Attention Metadata**

**æ–‡ä»¶**: `vllm/attention/backends/flash_attn.py`

```python
@dataclass
class FlashAttentionMetadata(AttentionMetadata):
    # ğŸ—ºï¸ æ ¸å¿ƒï¼šä¼ é€’ç»™CUDA kernelçš„block table tensor
    block_tables: Optional[torch.Tensor]  # shape: [batch_size, max_blocks_per_seq]
    
    # ç¤ºä¾‹tensor:
    # block_tables = torch.tensor([
    #     [10, 15, 23,  8,  0,  0],  # åºåˆ—1: 4ä¸ªæœ‰æ•ˆblocks + 2ä¸ªpadding
    #     [ 2,  7, 19,  0,  0,  0],  # åºåˆ—2: 3ä¸ªæœ‰æ•ˆblocks + 3ä¸ªpadding  
    #     [31, 12,  4, 18, 25,  0]   # åºåˆ—3: 5ä¸ªæœ‰æ•ˆblocks + 1ä¸ªpadding
    # ])
```

#### 4. **CUDAæ‰§è¡Œå±‚ - Attention Kernel**

**æ–‡ä»¶**: `csrc/attention/attention_kernels.cuh`

```cpp
__global__ void paged_attention_v1_kernel(
    scalar_t* __restrict__ out,
    const scalar_t* __restrict__ q,
    const cache_t* __restrict__ k_cache,
    const cache_t* __restrict__ v_cache,
    const int* __restrict__ block_tables,  // ğŸ—ºï¸ æ ¸å¿ƒï¼šblock tableæ•°ç»„
    const int* __restrict__ seq_lens,
    const int max_num_blocks_per_seq,
    // ... å…¶ä»–å‚æ•°
) {
    // ğŸ”¥ å…³é”®ï¼šæ ¹æ®block tableæŸ¥æ‰¾å®é™…çš„K/Væ•°æ®
    const int seq_idx = blockIdx.y;
    const int* seq_block_table = block_tables + seq_idx * max_num_blocks_per_seq;
    
    // éå†è¯¥åºåˆ—çš„æ‰€æœ‰blocks
    for (int block_idx = 0; block_idx < num_blocks; ++block_idx) {
        // ğŸ¯ é€šè¿‡block tableè·å–ç‰©ç†blockåœ°å€
        const int64_t physical_block_number = static_cast<int64_t>(seq_block_table[block_idx]);
        
        // ğŸ”— æ ¹æ®ç‰©ç†blockåœ°å€è®¿é—®K/V cache
        const cache_t* k_block_cache_ptr = 
            k_cache + physical_block_number * kv_block_stride + kv_head_idx * kv_head_stride;
        const cache_t* v_block_cache_ptr = 
            v_cache + physical_block_number * kv_block_stride + kv_head_idx * kv_head_stride;
            
        // ä½¿ç”¨K/Væ•°æ®è¿›è¡Œattentionè®¡ç®—...
    }
}
```

### ğŸ”„ Block Tableçš„å®Œæ•´å·¥ä½œæµç¨‹

```mermaid
graph TB
    subgraph "ğŸ¯ Stage 1: Block Tableåˆ›å»º"
        A1[æ–°åºåˆ—åˆ°è¾¾] --> A2[BlockManager.allocate]
        A2 --> A3[åˆ›å»ºBlockTableå¯¹è±¡]
        A3 --> A4[åˆ†é…ç‰©ç†blocks: 10,15,23,8]
        A4 --> A5[block_tableså­˜å‚¨: seq_id->BlockTable]
    end
    
    subgraph "ğŸ—ºï¸ Stage 2: é€»è¾‘åˆ°ç‰©ç†æ˜ å°„"
        B1[Tokenä½ç½®: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
        B2[é€»è¾‘Block: 0,0,0,0,1,1,1,1,2,2,2, 2, 3, 3, 3, 3]
        B3[ç‰©ç†Block: 10,10,10,10,15,15,15,15,23,23,23,23,8,8,8,8]
        B4[ç‰©ç†åœ°å€: 160,161,162,163,240,241,242,243,368,369,370,371,128,129,130,131]
        
        B1 --> B2
        B2 --> B3  
        B3 --> B4
    end
    
    subgraph "ğŸ“¦ Stage 3: æ•°æ®ä¼ é€’"
        C1[SequenceGroupMetadata] --> C2[block_tableså­—æ®µ]
        C2 --> C3[ä¼ é€’ç»™Worker]
        C3 --> C4[FlashAttentionMetadata]
        C4 --> C5[è½¬æ¢ä¸ºGPU tensor]
    end
    
    subgraph "âš¡ Stage 4: CUDAæ‰§è¡Œ"
        D1[Attention Kernelå¯åŠ¨] --> D2[seq_block_tableæŒ‡é’ˆ]
        D2 --> D3[éå†blocks: 0,1,2,3]
        D3 --> D4[æŸ¥è¡¨: 10,15,23,8]
        D4 --> D5[è®¿é—®K/V cacheç‰©ç†åœ°å€]
        D5 --> D6[æ‰§è¡Œattentionè®¡ç®—]
    end
    
    A5 --> C1
    B4 --> C2
    C5 --> D1
    
    style A3 fill:#e1f5fe
    style B3 fill:#ffecb3
    style C4 fill:#c8e6c9
    style D4 fill:#fce4ec
```

### ğŸ§® Block Tableçš„å…·ä½“æ˜ å°„è®¡ç®—

è®©æˆ‘ç”¨ä¸€ä¸ªå…·ä½“ä¾‹å­å±•ç¤ºBlock Tableå¦‚ä½•å·¥ä½œï¼š

#### **åœºæ™¯**: ä¸€ä¸ª16 tokensçš„åºåˆ—ï¼Œblock_size=4

```python
# ğŸ¯ è¾“å…¥æ•°æ®
sequence_tokens = [101, 234, 567, 890, 123, 456, 789, 012, 345, 678, 901, 234, 567, 890, 123, 456]
block_size = 4
sequence_length = 16

# ğŸ—ºï¸ Step 1: è®¡ç®—éœ€è¦çš„é€»è¾‘blocksæ•°é‡
num_blocks_needed = (sequence_length + block_size - 1) // block_size  # = 4

# ğŸ”„ Step 2: BlockManageråˆ†é…ç‰©ç†blocks
physical_blocks_allocated = [10, 15, 23, 8]  # ç”±allocatoråˆ†é…çš„ç‰©ç†block ID

# ğŸ“‹ Step 3: åˆ›å»ºBlock Tableæ˜ å°„
block_table = {
    # é€»è¾‘block_id -> ç‰©ç†block_id
    0: 10,  # å‰4ä¸ªtokens (0-3) å­˜å‚¨åœ¨ç‰©ç†block 10
    1: 15,  # ç¬¬5-8ä¸ªtokens (4-7) å­˜å‚¨åœ¨ç‰©ç†block 15  
    2: 23,  # ç¬¬9-12ä¸ªtokens (8-11) å­˜å‚¨åœ¨ç‰©ç†block 23
    3: 8    # æœ€å4ä¸ªtokens (12-15) å­˜å‚¨åœ¨ç‰©ç†block 8
}
```

#### **Slot Mappingè®¡ç®—** (é€»è¾‘åœ°å€â†’ç‰©ç†åœ°å€)

**æ–‡ä»¶**: `vllm/attention/backends/utils.py`

```python
def compute_slot_mapping(slot_mapping: List[int], seq_id: int, seq_len: int, 
                        block_size: int, block_tables: Dict[int, List[int]]):
    """è®¡ç®—æ¯ä¸ªtokençš„ç‰©ç†slotåœ°å€"""
    
    block_table = block_tables[seq_id]  # [10, 15, 23, 8]
    
    for token_position in range(seq_len):  # 0 to 15
        # ğŸ§® è®¡ç®—tokenå±äºå“ªä¸ªé€»è¾‘block
        logical_block_id = token_position // block_size
        
        # ğŸ” æŸ¥æ‰¾å¯¹åº”çš„ç‰©ç†block
        physical_block_id = block_table[logical_block_id]
        
        # ğŸ“ è®¡ç®—åœ¨blockå†…çš„åç§»
        block_offset = token_position % block_size
        
        # ğŸ¯ è®¡ç®—æœ€ç»ˆçš„ç‰©ç†slotåœ°å€
        physical_slot = physical_block_id * block_size + block_offset
        slot_mapping.append(physical_slot)

# ğŸ”¥ æ‰§è¡Œç»“æœ:
# token_position:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15]
# logical_block:   [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]
# physical_block:  [10,10,10,10,15,15,15,15,23,23,23,23, 8, 8, 8, 8]
# physical_slot:   [40,41,42,43,60,61,62,63,92,93,94,95,32,33,34,35]
```

### ğŸ¯ Attention Kernelä¸­çš„Block Tableä½¿ç”¨

å½“attention kernelæ‰§è¡Œæ—¶ï¼Œå®ƒé€šè¿‡block tableè¿›è¡Œå®é™…çš„å†…å­˜è®¿é—®ï¼š

```cpp
// ğŸ”¥ CUDA Kernelä¸­çš„å®é™…ä½¿ç”¨
__device__ void attention_compute(int seq_idx, int* block_tables) {
    // è·å–è¯¥åºåˆ—çš„block table
    const int* seq_block_table = block_tables + seq_idx * max_blocks_per_seq;
    
    // ğŸ—ºï¸ éå†åºåˆ—çš„æ‰€æœ‰blocksè¿›è¡Œattentionè®¡ç®—
    for (int logical_block_idx = 0; logical_block_idx < num_blocks; logical_block_idx++) {
        // ğŸ“ æŸ¥è¡¨ï¼šé€»è¾‘block -> ç‰©ç†block
        int physical_block_id = seq_block_table[logical_block_idx];
        
        // ğŸ¯ è®¿é—®K cache: ç‰©ç†åœ°å€ = ç‰©ç†block * block_stride + head_offset
        const cache_t* k_block_ptr = k_cache + physical_block_id * kv_block_stride + 
                                     kv_head_idx * kv_head_stride;
        
        // ğŸ¯ è®¿é—®V cache
        const cache_t* v_block_ptr = v_cache + physical_block_id * kv_block_stride + 
                                     kv_head_idx * kv_head_stride;
        
        // ğŸ§® ä½¿ç”¨å®é™…çš„K/Væ•°æ®è¿›è¡Œattentionè®¡ç®—
        compute_attention_scores(q_vector, k_block_ptr, v_block_ptr);
    }
}
```

### ğŸ”‘ Block Tableçš„å…³é”®ä¼˜åŠ¿

#### 1. **å†…å­˜ç¢ç‰‡åŒ–è§£å†³**
```
ä¼ ç»Ÿæ–¹å¼: [seq1_tokens][seq2_tokens][seq3_tokens] â†’ å†…å­˜ç¢ç‰‡
Pagedæ–¹å¼: ä½¿ç”¨Block Tableå°†é€»è¾‘è¿ç»­æ˜ å°„åˆ°ç‰©ç†ä¸è¿ç»­
```

#### 2. **åŠ¨æ€å†…å­˜ç®¡ç†**
```python
# Block Tableæ”¯æŒåŠ¨æ€å¢é•¿
initial_blocks = [10, 15]        # 2ä¸ªblocks
# åºåˆ—ç»§ç»­ç”Ÿæˆ...
expanded_blocks = [10, 15, 23, 8] # åŠ¨æ€æ·»åŠ æ–°blocks
```

#### 3. **é«˜æ•ˆçš„Copy-on-Write**
```python
# ä¸¤ä¸ªåºåˆ—å…±äº«ç›¸åŒçš„prefill blocks
seq1_blocks = [10, 15, 23]  # å…±äº«blocks 10, 15
seq2_blocks = [10, 15, 8]   # å…±äº«blocks 10, 15ï¼Œç‹¬æœ‰block 8
```

### ğŸ“Š Block Tableåœ¨ä¸åŒé˜¶æ®µçš„å½¢æ€

| é˜¶æ®µ | å½¢æ€ | ç¤ºä¾‹ | ä½œç”¨ |
|------|------|------|------|
| **Pythoné€»è¾‘å±‚** | `Dict[int, BlockTable]` | `{seq_id: BlockTableå¯¹è±¡}` | èµ„æºç®¡ç†å’Œè°ƒåº¦ |
| **å…ƒæ•°æ®ä¼ é€’** | `Dict[int, List[int]]` | `{123: [10,15,23,8]}` | Workeré—´é€šä¿¡ |
| **GPU Tensor** | `torch.Tensor` | `[[10,15,23,8,0,0]]` | é«˜æ•ˆGPUè®¿é—® |
| **CUDA Kernel** | `int* block_tables` | `[10,15,23,8,...]` | ç¡¬ä»¶çº§å†…å­˜è®¿é—® |

### ğŸ¯ æ€»ç»“ï¼šBlock Tableçš„æ ¸å¿ƒä½œç”¨

Block Tableå°±æ˜¯æ‚¨è¯´çš„"å†…å­˜table"ï¼Œå®ƒåœ¨vLLMä¸­ï¼š

1. **ğŸ—ºï¸ ç»´æŠ¤æ˜ å°„å…³ç³»**: é€»è¾‘åœ°å€ â†” ç‰©ç†åœ°å€
2. **ğŸ“ æ”¯æŒåŠ¨æ€åˆ†é…**: åºåˆ—å¯ä»¥åŠ¨æ€å¢é•¿
3. **âš¡ ä¼˜åŒ–å†…å­˜è®¿é—®**: é¿å…å†…å­˜ç¢ç‰‡å’Œæ‹·è´
4. **ğŸ”„ å®ç°CoWæœºåˆ¶**: é«˜æ•ˆçš„beam searchå’Œå¹¶è¡Œç”Ÿæˆ
5. **ğŸ¯ ç¡¬ä»¶çº§ä¼˜åŒ–**: ç›´æ¥åœ¨CUDA kernelä¸­ä½¿ç”¨

è¿™ä¸ªè®¾è®¡è®©vLLMèƒ½å¤Ÿé«˜æ•ˆåœ°ç®¡ç†å¤§é‡å¹¶å‘åºåˆ—çš„KV cacheï¼ŒåŒæ—¶ä¿æŒoptimal memory usageå’Œhigh throughputï¼