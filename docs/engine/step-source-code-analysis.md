# LLMEngine.step() æºä»£ç é€è¡Œè§£æ

## ä½ çš„ç†è§£æ˜¯å¯¹çš„ï¼

ä½ è¯´å¾—å®Œå…¨æ­£ç¡®ï¼š**æ¯è°ƒç”¨ä¸€æ¬¡stepåŸºæœ¬å°±ç”Ÿæˆnext tokenï¼Œç„¶åå¤–éƒ¨ä¸æ–­å¾ªç¯**ã€‚

è®©æˆ‘æŠŠå®Œæ•´çš„æºä»£ç è´´å‡ºæ¥ï¼Œé€è¡Œè¯¦ç»†è§£é‡Šï¼š

## å®Œæ•´æºä»£ç 

```python
def step(self) -> List[Union[RequestOutput, PoolingRequestOutput]]:
    """Performs one decoding iteration and returns newly generated results."""
    
    # ç¬¬1-3è¡Œï¼šæ£€æŸ¥pipelineå¹¶è¡Œé™åˆ¶
    if self.parallel_config.pipeline_parallel_size > 1:
        raise NotImplementedError(
            "Pipeline parallelism is only supported through AsyncLLMEngine "
            "as performance will be severely degraded otherwise.")

    # ç¬¬4-5è¡Œï¼šè®¾ç½®è™šæ‹Ÿå¼•æ“IDï¼ˆå•è¿›ç¨‹æ—¶æ€»æ˜¯0ï¼‰
    virtual_engine = 0

    # ç¬¬6-10è¡Œï¼šè·å–ç¼“å­˜çš„è°ƒåº¦è¾“å‡ºï¼ˆç”¨äºmulti-stepï¼‰
    cached_outputs = self.cached_scheduler_outputs[virtual_engine]
    seq_group_metadata_list = cached_outputs.seq_group_metadata_list
    scheduler_outputs = cached_outputs.scheduler_outputs
    allow_async_output_proc = cached_outputs.allow_async_output_proc

    # ç¬¬11è¡Œï¼šè·å–è°ƒåº¦ä¸Šä¸‹æ–‡
    ctx = self.scheduler_contexts[virtual_engine]

    # ç¬¬12è¡Œï¼šæ¸…ç©ºä¸Šä¸€è½®çš„è¾“å‡º
    ctx.request_outputs.clear()

    # ç¬¬13-20è¡Œï¼šå†³å®šæ˜¯å¦éœ€è¦é‡æ–°è°ƒåº¦
    if not self._has_remaining_steps(seq_group_metadata_list) and not self._skip_scheduling_next_step:
        # ğŸ”¥ æ ¸å¿ƒè°ƒåº¦é€»è¾‘ï¼šé€‰æ‹©è¦æ‰§è¡Œçš„è¯·æ±‚
        (seq_group_metadata_list, scheduler_outputs, allow_async_output_proc) = self.scheduler[virtual_engine].schedule()

        # ä¿å­˜è°ƒåº¦ç»“æœåˆ°ä¸Šä¸‹æ–‡
        ctx.seq_group_metadata_list = seq_group_metadata_list
        ctx.scheduler_outputs = scheduler_outputs

        # ç¬¬21-27è¡Œï¼šæ¸…ç†å·²å®Œæˆçš„è¯·æ±‚
        finished_requests_ids = self.scheduler[virtual_engine].get_and_reset_finished_requests_ids()
        for finished_request_id in finished_requests_ids:
            if finished_request_id in self.seq_id_to_seq_group:
                del self.seq_id_to_seq_group[finished_request_id]

        # ç¬¬28-30è¡Œï¼šå¤„ç†å¼‚æ­¥è¾“å‡ºé˜Ÿåˆ—
        if not allow_async_output_proc and len(ctx.output_queue) > 0:
            self._process_model_outputs(ctx=ctx)

        # ç¬¬31-36è¡Œï¼šMulti-stepç¼“å­˜å¤„ç†
        if (self.scheduler_config.is_multi_step and scheduler_outputs.num_lookahead_slots > 0):
            self._cache_scheduler_outputs_for_multi_step(
                virtual_engine, seq_group_metadata_list, scheduler_outputs, allow_async_output_proc)
    else:
        finished_requests_ids = list()

    # ç¬¬37-38è¡Œï¼šç¡®ä¿è°ƒåº¦ç»“æœä¸ä¸ºç©º
    assert seq_group_metadata_list is not None
    assert scheduler_outputs is not None

    # ç¬¬39-65è¡Œï¼šæ¨¡å‹æ‰§è¡Œé˜¶æ®µ
    if not scheduler_outputs.is_empty():
        # è·å–ä¸Šä¸€è½®çš„é‡‡æ ·tokenï¼ˆç”¨äºpipelineå¹¶è¡Œï¼‰
        last_sampled_token_ids = self._get_last_sampled_token_ids(virtual_engine)

        # ğŸ”¥ æ„é€ æ¨¡å‹æ‰§è¡Œè¯·æ±‚
        execute_model_req = ExecuteModelRequest(
            seq_group_metadata_list=seq_group_metadata_list,
            blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,
            blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,
            blocks_to_copy=scheduler_outputs.blocks_to_copy,
            num_lookahead_slots=scheduler_outputs.num_lookahead_slots,
            running_queue_size=scheduler_outputs.running_queue_size,
            finished_requests_ids=finished_requests_ids,
            last_sampled_token_ids=last_sampled_token_ids)

        # è®¾ç½®å¼‚æ­¥å›è°ƒ
        if allow_async_output_proc:
            execute_model_req.async_callback = self.async_callbacks[virtual_engine]

        try:
            # ğŸ”¥ğŸ”¥ğŸ”¥ æœ€å…³é”®çš„ä¸€è¡Œï¼šæ‰§è¡Œæ¨¡å‹æ¨ç†ï¼
            outputs = self.model_executor.execute_model(execute_model_req=execute_model_req)
            self._skip_scheduling_next_step = False
        except InputProcessingError as e:
            # å¤„ç†è¾“å…¥é”™è¯¯
            invalid_request_id = e.request_id
            self._abort_and_cache_schedule(...)
            raise

        # Multi-stepæƒ…å†µä¸‹æ›´æ–°ç¼“å­˜
        if self.scheduler_config.is_multi_step:
            self._update_cached_scheduler_output(virtual_engine, outputs)
    else:
        # æ²¡æœ‰è¦æ‰§è¡Œçš„è¯·æ±‚ï¼Œå¤„ç†å¾…å¤„ç†çš„è¾“å‡º
        if len(ctx.output_queue) > 0:
            self._process_model_outputs(ctx=ctx)
        outputs = []

    # ç¬¬66-68è¡Œï¼šå®Œæˆå½“å‰step
    if self.scheduler_config.is_multi_step:
        for seq_group in seq_group_metadata_list:
            seq_group.finish_step()

    # ç¬¬69-95è¡Œï¼šè¾“å‡ºå¤„ç†é˜¶æ®µ
    if not self._has_remaining_steps(seq_group_metadata_list):
        # æ¸…ç†multi-stepç¼“å­˜
        if self.scheduler_config.is_multi_step:
            self.cached_scheduler_outputs[0] = SchedulerOutputState()

        # åˆ¤æ–­æ˜¯å¦æ˜¯ç¬¬ä¸€æ­¥è¾“å‡º
        is_first_step_output: bool = False if not seq_group_metadata_list \
            else seq_group_metadata_list[0].state.num_steps == 1

        # ğŸ”¥ å°†è¾“å‡ºæ·»åŠ åˆ°é˜Ÿåˆ—
        ctx.append_output(outputs=outputs,
                          seq_group_metadata_list=seq_group_metadata_list,
                          
                          is_last_step=True,
                          is_first_step_output=is_first_step_output)

        # å¼‚æ­¥å¤„ç†æƒ…å†µ
        if outputs and allow_async_output_proc:
            assert len(outputs) == 1, ("Async postprocessor expects only a single output set")
            self._advance_to_next_step(outputs[0], seq_group_metadata_list, scheduler_outputs.scheduled_seq_groups)

        # åŒæ­¥å¤„ç†æƒ…å†µ
        if not allow_async_output_proc:
            # ğŸ”¥ å¤„ç†æ¨¡å‹è¾“å‡ºï¼šè§£ç tokenã€æ›´æ–°çŠ¶æ€ã€ç”ŸæˆRequestOutput
            self._process_model_outputs(ctx=ctx)
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            self.do_log_stats(scheduler_outputs, outputs)
            # è¿½è¸ªä¿¡æ¯
            self.do_tracing(scheduler_outputs)
    else:
        # Multi-stepæƒ…å†µç›´æ¥è¿”å›
        return ctx.request_outputs

    # ç¬¬96-105è¡Œï¼šæ¸…ç†å·¥ä½œ
    if not self.has_unfinished_requests():
        # å¤„ç†å‰©ä½™çš„å¼‚æ­¥è¾“å‡º
        if len(ctx.output_queue) > 0:
            self._process_model_outputs(ctx=ctx)
        assert len(ctx.output_queue) == 0
        # åœæ­¢è¿œç¨‹workerçš„æ‰§è¡Œå¾ªç¯
        logger.debug("Stopping remote worker execution loop.")
        self.model_executor.stop_remote_worker_execution_loop()

    # ç¬¬106è¡Œï¼šè¿”å›æœ€ç»ˆç»“æœ
    return ctx.request_outputs
```

## è¯¦ç»†ç¤ºä¾‹åˆ†æï¼šæ¯è¡Œä»£ç çš„å…·ä½“åŠŸèƒ½

### ğŸš€ ç¬¬1é˜¶æ®µï¼šåˆå§‹åŒ–å’Œæ£€æŸ¥ (ç¬¬1-12è¡Œ)

#### ç¤ºä¾‹åœºæ™¯ï¼š
å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç”¨æˆ·è¯·æ±‚ï¼š"å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"

```python
# ç”¨æˆ·ä»£ç åˆå§‹åŒ–
engine = LLMEngine.from_engine_args(args)
request_id = "poem_req_001"
prompt = "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"
sampling_params = SamplingParams(max_tokens=100, temperature=0.8)
engine.add_request(request_id, prompt, sampling_params)

# ç°åœ¨è°ƒç”¨ç¬¬ä¸€æ¬¡step()
outputs = engine.step()
```

**ç¬¬1-3è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
# æ£€æŸ¥pipelineå¹¶è¡Œé…ç½®
if self.parallel_config.pipeline_parallel_size > 1:
    raise NotImplementedError(...)

# å®é™…çŠ¶æ€ï¼š
# self.parallel_config.pipeline_parallel_size = 1 (å•GPUæ¨¡å¼)
# âœ“ ç»§ç»­æ‰§è¡Œï¼Œä¸æŠ›å¼‚å¸¸
```

**ç¬¬4-5è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
virtual_engine = 0

# å®é™…çŠ¶æ€ï¼š
# virtual_engine = 0  (å•è¿›ç¨‹æ¨¡å¼ï¼Œåªæœ‰ä¸€ä¸ªè™šæ‹Ÿå¼•æ“)
```

**ç¬¬6-10è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
# è·å–ç¼“å­˜çŠ¶æ€ï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ä¸ºç©ºï¼‰
cached_outputs = self.cached_scheduler_outputs[0]
# cached_outputs = SchedulerOutputState(
#     seq_group_metadata_list=None,
#     scheduler_outputs=None, 
#     allow_async_output_proc=False
# )

seq_group_metadata_list = None  # ç¬¬ä¸€æ¬¡ä¸ºç©º
scheduler_outputs = None        # ç¬¬ä¸€æ¬¡ä¸ºç©º  
allow_async_output_proc = False # ç¬¬ä¸€æ¬¡ä¸ºFalse
```

**ç¬¬11-12è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
ctx = self.scheduler_contexts[0]
# ctx = SchedulerContext(
#     request_outputs=[],  # ç©ºåˆ—è¡¨
#     output_queue=[],     # ç©ºé˜Ÿåˆ—
#     seq_group_metadata_list=None,
#     scheduler_outputs=None
# )

ctx.request_outputs.clear()  # æ¸…ç©ºè¾“å‡ºåˆ—è¡¨ï¼ˆç¬¬ä¸€æ¬¡æœ¬æ¥å°±æ˜¯ç©ºçš„ï¼‰
```

### ğŸ¯ ç¬¬2é˜¶æ®µï¼šè°ƒåº¦å†³ç­– (ç¬¬13-36è¡Œ)

**ç¬¬13-15è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
# æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è°ƒåº¦
if not self._has_remaining_steps(None) and not self._skip_scheduling_next_step:
#      â†‘ è¿”å›Trueï¼ˆæ²¡æœ‰å‰©ä½™æ­¥éª¤ï¼‰    â†‘ è¿”å›Falseï¼ˆä¸è·³è¿‡è°ƒåº¦ï¼‰
#      æ•´ä¸ªæ¡ä»¶ = Trueï¼Œéœ€è¦é‡æ–°è°ƒåº¦
```

**ç¬¬16è¡Œï¼šğŸ”¥ æ ¸å¿ƒè°ƒåº¦ç¤ºä¾‹**
```python
(seq_group_metadata_list, scheduler_outputs, allow_async_output_proc) = \
    self.scheduler[0].schedule()

# è°ƒåº¦å™¨å†…éƒ¨å‘ç”Ÿä»€ä¹ˆï¼š
# 1. æ£€æŸ¥WAITINGé˜Ÿåˆ—ï¼Œå‘ç°"poem_req_001"ç­‰å¾…å¤„ç†
# 2. æ£€æŸ¥GPUå†…å­˜ï¼Œæœ‰è¶³å¤Ÿç©ºé—´è¿›è¡Œprefill
# 3. ä»WAITINGç§»åŠ¨åˆ°RUNNINGé˜Ÿåˆ—

# è¿”å›ç»“æœï¼š
seq_group_metadata_list = [
    SequenceGroupMetadata(
        request_id="poem_req_001",
        is_prompt=True,  # è¿™æ˜¯prefillé˜¶æ®µ
        seq_data={
            0: SequenceData(
                seq_id=0,
                prompt="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
                prompt_token_ids=[123, 456, 789, 234, 567, 890, 345],  # tokenized
                output_token_ids=[],  # è¿˜æ²¡æœ‰è¾“å‡º
                get_len=lambda: 7  # prompté•¿åº¦
            )
        },
        sampling_params=SamplingParams(max_tokens=100, temperature=0.8),
        block_tables={0: [0, 1]},  # åˆ†é…çš„å†…å­˜å—
        do_sample=True,
        pooling_params=None,
        lora_request=None
    )
]

scheduler_outputs = SchedulerOutputs(
    scheduled_seq_groups=[seq_group_metadata_list[0]],  # è¦æ‰§è¡Œçš„åºåˆ—ç»„
    num_prefill_groups=1,     # 1ä¸ªprefillä»»åŠ¡
    num_generation_groups=0,  # 0ä¸ªgenerationä»»åŠ¡
    blocks_to_swap_in={},     # æ— éœ€swap in
    blocks_to_swap_out={},    # æ— éœ€swap out  
    blocks_to_copy={},        # æ— éœ€copy
    ignored_seq_groups=[],    # æ— å¿½ç•¥çš„åºåˆ—
    num_lookahead_slots=0,    # émulti-stepæ¨¡å¼
    running_queue_size=1      # runningé˜Ÿåˆ—å¤§å°ä¸º1
)

allow_async_output_proc = False  # åŒæ­¥å¤„ç†æ¨¡å¼
```

**ç¬¬18-20è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
# ä¿å­˜è°ƒåº¦ç»“æœåˆ°ä¸Šä¸‹æ–‡
ctx.seq_group_metadata_list = seq_group_metadata_list
ctx.scheduler_outputs = scheduler_outputs

# å®é™…çŠ¶æ€ï¼š
# ctx ç°åœ¨åŒ…å«äº†è¦æ‰§è¡Œçš„åºåˆ—ç»„ä¿¡æ¯
```

**ç¬¬21-27è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
# è·å–å·²å®Œæˆçš„è¯·æ±‚IDï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ä¸ºç©ºï¼‰
finished_requests_ids = []  # ç©ºåˆ—è¡¨ï¼Œå› ä¸ºè¿˜æ²¡æœ‰å®Œæˆçš„è¯·æ±‚

# å¾ªç¯å¤„ç†ï¼ˆè¿™æ¬¡ä¸ºç©ºï¼Œä¸æ‰§è¡Œï¼‰
for finished_request_id in finished_requests_ids:
    # ä¸æ‰§è¡Œï¼Œå› ä¸ºåˆ—è¡¨ä¸ºç©º
    pass
```

### âš¡ ç¬¬3é˜¶æ®µï¼šæ¨¡å‹æ‰§è¡Œ (ç¬¬39-65è¡Œ)

**ç¬¬39è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
if not scheduler_outputs.is_empty():
    # scheduler_outputs.is_empty() æ£€æŸ¥ï¼š
    # - scheduled_seq_groupsæ˜¯å¦ä¸ºç©º -> [seq_group] ä¸ä¸ºç©º
    # - è¿”å›Falseï¼Œæ‰€ä»¥æ¡ä»¶ä¸ºTrueï¼Œç»§ç»­æ‰§è¡Œ
```

**ç¬¬41-42è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
last_sampled_token_ids = self._get_last_sampled_token_ids(0)
# ç¬¬ä¸€æ¬¡prefillï¼Œè¿”å›ç©ºå­—å…¸ï¼š{}
```

**ç¬¬43-53è¡Œï¼šæ„é€ æ‰§è¡Œè¯·æ±‚ç¤ºä¾‹**
```python
execute_model_req = ExecuteModelRequest(
    seq_group_metadata_list=[seq_group_metadata],  # åŒ…å«æˆ‘ä»¬çš„è¯—æ­Œè¯·æ±‚
    blocks_to_swap_in={},      # ç©ºå­—å…¸ï¼Œæ— éœ€ä»CPU swapåˆ°GPU
    blocks_to_swap_out={},     # ç©ºå­—å…¸ï¼Œæ— éœ€ä»GPU swapåˆ°CPU
    blocks_to_copy={},         # ç©ºå­—å…¸ï¼Œæ— éœ€å¤åˆ¶å†…å­˜å—
    num_lookahead_slots=0,     # émulti-stepæ¨¡å¼
    running_queue_size=1,      # å½“å‰runningé˜Ÿåˆ—ä¸­æœ‰1ä¸ªè¯·æ±‚
    finished_requests_ids=[],  # ç©ºåˆ—è¡¨ï¼Œæ— å·²å®Œæˆè¯·æ±‚
    last_sampled_token_ids={}  # ç©ºå­—å…¸ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡æ‰§è¡Œ
)

# è®¾ç½®å¼‚æ­¥å›è°ƒï¼ˆæˆ‘ä»¬çš„ä¾‹å­ä¸­ä¸ºFalseï¼Œè·³è¿‡ï¼‰
# if allow_async_output_proc:  # Falseï¼Œä¸æ‰§è¡Œ
#     execute_model_req.async_callback = ...
```

**ç¬¬58è¡Œï¼šğŸ”¥ğŸ”¥ğŸ”¥ æœ€å…³é”®çš„æ‰§è¡Œç¤ºä¾‹**
```python
outputs = self.model_executor.execute_model(execute_model_req=execute_model_req)

# æ¨¡å‹æ‰§è¡Œå†…éƒ¨å‘ç”Ÿä»€ä¹ˆï¼š
# 1. å†…å­˜ç®¡ç†ï¼šæ— éœ€swapï¼Œå› ä¸ºblocks_to_swap_in/outéƒ½ä¸ºç©º
# 2. å‡†å¤‡è¾“å…¥å¼ é‡ï¼š
#    - input_ids = torch.tensor([[123, 456, 789, 234, 567, 890, 345]])  # è¯—æ­Œprompt
#    - attention_mask, position_idsç­‰
# 3. æ¨¡å‹å‰å‘ä¼ æ’­ï¼š
#    - hidden_states = self.model.embed_tokens(input_ids)
#    - for layer in self.model.layers: hidden_states = layer(hidden_states, ...)
#    - logits = self.model.lm_head(hidden_states)  # [1, 7, vocab_size]
# 4. é‡‡æ ·ï¼šä»æœ€åä¸€ä¸ªä½ç½®çš„logitsé‡‡æ ·
#    - next_token_logits = logits[0, -1, :]  # å–æœ€åä¸€ä¸ªä½ç½®
#    - next_token_id = sample(next_token_logits, temperature=0.8)  # å‡è®¾é‡‡æ ·å¾—åˆ°token_id=12345 ("æ˜¥")
# 5. æ›´æ–°KV cacheï¼šä¿å­˜attentionçš„key-valueçŠ¶æ€

# è¿”å›ç»“æœï¼š
outputs = [
    SamplerOutput(
        outputs=[
            CompletionSequenceGroupOutput(
                samples=[
                    SequenceOutput(
                        parent_seq_id=0,
                        output_token=12345,  # token_id for "æ˜¥"
                        logprobs={12345: -0.5, 23456: -1.2, ...}  # top tokensçš„æ¦‚ç‡
                    )
                ],
                prompt_logprobs=None
            )
        ],
        sampled_token_probs=None,
        sampled_token_ids=[12345],  # æ–°ç”Ÿæˆçš„token
        spec_decode_worker_metrics=None
    )
]

# è®¾ç½®è·³è¿‡æ ‡å¿—
self._skip_scheduling_next_step = False
```

### ğŸ¨ ç¬¬4é˜¶æ®µï¼šè¾“å‡ºå¤„ç† (ç¬¬69-95è¡Œ)

**ç¬¬72-73è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
# æ¸…ç†multi-stepç¼“å­˜ï¼ˆæˆ‘ä»¬çš„ä¾‹å­ä¸­ä¸ä½¿ç”¨multi-stepï¼‰
if self.scheduler_config.is_multi_step:  # Falseï¼Œè·³è¿‡
    pass

# åˆ¤æ–­æ˜¯å¦æ˜¯ç¬¬ä¸€æ­¥è¾“å‡º
is_first_step_output: bool = False if not seq_group_metadata_list \
    else seq_group_metadata_list[0].state.num_steps == 1
# seq_group_metadata_list[0].state.num_steps = 1ï¼ˆç¬¬ä¸€æ­¥ï¼‰
# æ‰€ä»¥ is_first_step_output = True
```

**ç¬¬79-85è¡Œï¼šæ·»åŠ è¾“å‡ºåˆ°é˜Ÿåˆ—ç¤ºä¾‹**
```python
ctx.append_output(
    outputs=outputs,  # åŒ…å«æ–°ç”Ÿæˆtoken "æ˜¥" çš„SamplerOutput
    seq_group_metadata_list=seq_group_metadata_list,  # åºåˆ—ç»„ä¿¡æ¯
    scheduler_outputs=scheduler_outputs,  # è°ƒåº¦è¾“å‡º
    is_async=False,   # åŒæ­¥å¤„ç†
    is_last_step=True,    # è¿™æ˜¯å®Œæ•´çš„ä¸€æ­¥
    is_first_step_output=True  # è¿™æ˜¯ç¬¬ä¸€æ­¥è¾“å‡º
)

# å†…éƒ¨å‘ç”Ÿä»€ä¹ˆï¼š
# ctx.output_queue.append(OutputContext(
#     outputs=outputs,
#     seq_group_metadata_list=seq_group_metadata_list,
#     ...
# ))
```

**ç¬¬91è¡Œï¼šğŸ”¥ å¤„ç†æ¨¡å‹è¾“å‡ºç¤ºä¾‹**
```python
self._process_model_outputs(ctx=ctx)

# å†…éƒ¨å‘ç”Ÿä»€ä¹ˆï¼š
# 1. ä»é˜Ÿåˆ—å–å‡ºoutputï¼š
#    output_ctx = ctx.output_queue.pop(0)
#    
# 2. è§£ç tokenï¼š
#    token_id = 12345
#    token_text = tokenizer.decode([token_id])  # "æ˜¥"
#    
# 3. æ›´æ–°åºåˆ—çŠ¶æ€ï¼š
#    seq_data.output_token_ids.append(12345)
#    seq_data.get_len() -> 8 (7ä¸ªprompt + 1ä¸ªoutput)
#    
# 4. æ£€æŸ¥åœæ­¢æ¡ä»¶ï¼š
#    if token_id == eos_token_id:  # æ£€æŸ¥æ˜¯å¦ç»“æŸ
#        seq.status = SequenceStatus.FINISHED_STOPPED
#    elif len(seq_data.output_token_ids) >= sampling_params.max_tokens:
#        seq.status = SequenceStatus.FINISHED_LENGTH
#    else:
#        seq.status = SequenceStatus.RUNNING  # ç»§ç»­è¿è¡Œ
#        
# 5. ç”ŸæˆRequestOutputï¼š
#    request_output = RequestOutput(
#        request_id="poem_req_001",
#        prompt="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
#        prompt_token_ids=[123, 456, 789, 234, 567, 890, 345],
#        outputs=[
#            CompletionOutput(
#                index=0,
#                text="æ˜¥",  # åˆ°ç›®å‰ä¸ºæ­¢ç”Ÿæˆçš„æ–‡æœ¬
#                token_ids=[12345],  # ç”Ÿæˆçš„token ids
#                cumulative_logprob=-0.5,
#                logprobs=LogProbs(...),
#                finish_reason=None,  # è¿˜æœªç»“æŸ
#                stop_reason=None
#            )
#        ],
#        finished=False,  # è¯·æ±‚è¿˜æœªå®Œæˆ
#        metrics=RequestMetrics(...)
#    )
#    
# 6. æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ï¼š
#    ctx.request_outputs.append(request_output)
```

**ç¬¬92-94è¡Œä»£ç ç¤ºä¾‹ï¼š**
```python
# è®°å½•ç»Ÿè®¡ä¿¡æ¯
self.do_log_stats(scheduler_outputs, outputs)
# è¾“å‡ºç±»ä¼¼ï¼š
# INFO:     Avg prompt throughput: 1000.0 tokens/s, generation throughput: 50.0 tokens/s

# è¿½è¸ªä¿¡æ¯ï¼ˆå¦‚æœå¼€å¯ï¼‰
self.do_tracing(scheduler_outputs)
# è®°å½•è¯¦ç»†çš„æ‰§è¡Œtraceï¼Œç”¨äºæ€§èƒ½åˆ†æ
```

**ç¬¬106è¡Œï¼šè¿”å›ç»“æœç¤ºä¾‹**
```python
return ctx.request_outputs

# è¿”å›ï¼š
# [
#     RequestOutput(
#         request_id="poem_req_001",
#         prompt="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—", 
#         outputs=[
#             CompletionOutput(
#                 text="æ˜¥",  # ç¬¬ä¸€æ¬¡stepç”Ÿæˆçš„æ–‡æœ¬
#                 token_ids=[12345],
#                 finish_reason=None,  # æœªå®Œæˆ
#                 ...
#             )
#         ],
#         finished=False  # è¯·æ±‚æœªå®Œæˆï¼Œéœ€è¦ç»§ç»­
#     )
# ]
```

## å®Œæ•´çš„å¤šæ­¥æ‰§è¡Œç¤ºä¾‹

è®©æˆ‘å±•ç¤ºè¿ç»­å‡ æ¬¡`step()`è°ƒç”¨çš„å®Œæ•´è¿‡ç¨‹ï¼š

```python
# åˆå§‹åŒ–
engine = LLMEngine.from_engine_args(args)
engine.add_request("poem_req_001", "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—", 
                  SamplingParams(max_tokens=20, temperature=0.8))

# === ç¬¬1æ¬¡step()ï¼šPrefillé˜¶æ®µ ===
print("=== Step 1: Prefill ===")
outputs1 = engine.step()
# è°ƒåº¦å™¨ï¼šé€‰æ‹©è¯·æ±‚è¿›è¡Œprefill
# æ¨¡å‹ï¼šå¤„ç†å®Œæ•´prompt "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"ï¼Œç”Ÿæˆç¬¬1ä¸ªtoken "æ˜¥"
# è¾“å‡ºï¼šRequestOutput(text="æ˜¥", finished=False)
print(f"Step 1 output: {outputs1[0].outputs[0].text}")  # "æ˜¥"

# === ç¬¬2æ¬¡step()ï¼šDecodeé˜¶æ®µ ===  
print("=== Step 2: Decode ===")
outputs2 = engine.step()
# è°ƒåº¦å™¨ï¼šç»§ç»­å¤„ç†åŒä¸€è¯·æ±‚ï¼ˆç°åœ¨æ˜¯decodeæ¨¡å¼ï¼‰
# æ¨¡å‹ï¼šåŸºäº"å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—æ˜¥"çš„contextï¼Œç”Ÿæˆç¬¬2ä¸ªtoken "å¤©"
# è¾“å‡ºï¼šRequestOutput(text="æ˜¥å¤©", finished=False)
print(f"Step 2 output: {outputs2[0].outputs[0].text}")  # "æ˜¥å¤©"

# === ç¬¬3æ¬¡step()ï¼šç»§ç»­Decode ===
print("=== Step 3: Decode ===")
outputs3 = engine.step()
# æ¨¡å‹ï¼šåŸºäºæ›´é•¿contextï¼Œç”Ÿæˆç¬¬3ä¸ªtoken "æ¥"
# è¾“å‡ºï¼šRequestOutput(text="æ˜¥å¤©æ¥", finished=False)
print(f"Step 3 output: {outputs3[0].outputs[0].text}")  # "æ˜¥å¤©æ¥"

# === ç»§ç»­å¾ªç¯ç›´åˆ°å®Œæˆ ===
step_count = 3
while engine.has_unfinished_requests():
    step_count += 1
    outputs = engine.step()
    if outputs:
        print(f"Step {step_count} output: {outputs[0].outputs[0].text}")
        if outputs[0].finished:
            print(f"å®Œæˆï¼æœ€ç»ˆæ–‡æœ¬: {outputs[0].outputs[0].text}")
            break

# å¯èƒ½çš„æœ€ç»ˆè¾“å‡ºï¼š
# "æ˜¥å¤©æ¥äº†ï¼Œä¸‡ç‰©å¤è‹ï¼Œ
#  ç»¿è‰å¦‚èŒµèŠ±æ»¡æï¼Œ
#  è¶èˆèœ‚é£é¸Ÿå•å•¾ï¼Œ
#  å¤§åœ°æ¢ä¸Šæ–°ç»¿è¡£ã€‚"
```

## å†…å­˜å’Œæ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹

**æ‰¹å¤„ç†å¤„ç†å¤šä¸ªè¯·æ±‚ï¼š**
```python
# åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚
engine.add_request("req1", "å†™ä¸€é¦–æ˜¥å¤©çš„è¯—", SamplingParams(max_tokens=50))
engine.add_request("req2", "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½", SamplingParams(max_tokens=100))
engine.add_request("req3", "What is machine learning?", SamplingParams(max_tokens=80))

# ä¸€æ¬¡step()åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚
outputs = engine.step()

# è°ƒåº¦å™¨æ™ºèƒ½å†³ç­–ï¼š
# - req1: è¿›è¡Œprefillï¼ˆå¤„ç†"å†™ä¸€é¦–æ˜¥å¤©çš„è¯—"ï¼‰
# - req2: è¿›è¡Œprefillï¼ˆå¤„ç†"ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"ï¼‰  
# - req3: ç­‰å¾…ä¸‹ä¸€è½®ï¼ˆGPUå†…å­˜ä¸è¶³ï¼‰

# æ¨¡å‹æ‰§è¡Œï¼š
# - æ‰¹é‡å¤„ç†req1å’Œreq2çš„prefill
# - é«˜æ•ˆåˆ©ç”¨GPUå¹¶è¡Œæ€§
# - ä¸€æ¬¡forward passå¤„ç†å¤šä¸ªåºåˆ—

# è¾“å‡ºï¼š
# [
#     RequestOutput(request_id="req1", outputs=[CompletionOutput(text="æ˜¥")]),
#     RequestOutput(request_id="req2", outputs=[CompletionOutput(text="äººå·¥")])
# ]
```

**å†…å­˜ç®¡ç†ç¤ºä¾‹ï¼š**
```python
# å½“GPUå†…å­˜ä¸è¶³æ—¶
outputs = engine.step()

# è°ƒåº¦å™¨å†³ç­–ï¼š
scheduler_outputs = SchedulerOutputs(
    scheduled_seq_groups=[...],
    blocks_to_swap_out={
        "req_old_1": [BlockTable([0, 1, 2])],  # å°†æ—§è¯·æ±‚swapåˆ°CPU
        "req_old_2": [BlockTable([3, 4, 5])]
    },
    blocks_to_swap_in={
        "req_new_1": [BlockTable([6, 7, 8])]   # å°†æ–°è¯·æ±‚swapåˆ°GPU
    },
    blocks_to_copy={
        "req_continue": [BlockCopy(src=9, dst=10)]  # å¤åˆ¶å…±äº«prefix
    }
)

# æ‰§è¡Œæ—¶ï¼š
# 1. å…ˆæ‰§è¡Œå†…å­˜æ“ä½œï¼šswap_out â†’ swap_in â†’ copy
# 2. å†æ‰§è¡Œæ¨¡å‹æ¨ç†
# 3. ç¡®ä¿å†…å­˜ä½¿ç”¨æœ€ä¼˜åŒ–
```

## å…³é”®ç†è§£ï¼šæ¯æ¬¡stepç¡®å®ç”Ÿæˆnext token

ä½ çš„ç†è§£å®Œå…¨æ­£ç¡®ï¼è®©æˆ‘ç”¨ä¸€ä¸ªå…·ä½“ä¾‹å­è¯´æ˜ï¼š

```python
# ç”¨æˆ·ä»£ç 
engine = LLMEngine.from_engine_args(args)
engine.add_request("req1", "Hello", SamplingParams(max_tokens=10))

# ç¬¬1æ¬¡è°ƒç”¨step()
outputs = engine.step()  
# å†…éƒ¨å‘ç”Ÿï¼š
# 1. è°ƒåº¦å™¨é€‰æ‹©"req1"è¿›è¡Œprefill
# 2. æ¨¡å‹å¤„ç†"Hello"ï¼Œç”Ÿæˆç¬¬1ä¸ªtokenï¼Œæ¯”å¦‚"world"
# 3. è¿”å›RequestOutputï¼ŒåŒ…å«"world"

# ç¬¬2æ¬¡è°ƒç”¨step()  
outputs = engine.step()
# å†…éƒ¨å‘ç”Ÿï¼š
# 1. è°ƒåº¦å™¨ç»§ç»­å¤„ç†"req1"ï¼ˆç°åœ¨æ˜¯decodeæ¨¡å¼ï¼‰
# 2. æ¨¡å‹åŸºäº"Hello world"çš„contextï¼Œç”Ÿæˆç¬¬2ä¸ªtokenï¼Œæ¯”å¦‚"!"
# 3. è¿”å›RequestOutputï¼ŒåŒ…å«"world!"

# ç¬¬3æ¬¡è°ƒç”¨step()
outputs = engine.step()
# ç»§ç»­ç”Ÿæˆä¸‹ä¸€ä¸ªtoken...

# å¤–éƒ¨å¾ªç¯
while engine.has_unfinished_requests():
    outputs = engine.step()  # æ¯æ¬¡ç”Ÿæˆnext token
    for output in outputs:
        if output.finished:
            print(f"å®Œæˆ: {output.outputs[0].text}")
```

## æ ¸å¿ƒè¦ç‚¹æ€»ç»“

1. **æ¯æ¬¡step = ä¸€æ¬¡å®Œæ•´æ¨ç†è¿­ä»£**
   - å¯èƒ½æ˜¯prefillï¼ˆå¤„ç†promptï¼‰
   - å¯èƒ½æ˜¯decodeï¼ˆç”Ÿæˆnext tokenï¼‰
   - å¯èƒ½åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚

2. **å¤–éƒ¨å¾ªç¯é©±åŠ¨**
   ```python
   while engine.has_unfinished_requests():
       outputs = engine.step()  # ç”Ÿæˆnext token
   ```

3. **æ‰¹å¤„ç†ä¼˜åŒ–**
   - ä¸€æ¬¡stepå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚
   - æœ‰äº›åšprefillï¼Œæœ‰äº›åšdecode
   - å……åˆ†åˆ©ç”¨GPUå¹¶è¡Œèƒ½åŠ›

4. **å†…å­˜ç®¡ç†**
   - æ¯æ¬¡stepéƒ½å¯èƒ½æ¶‰åŠå†…å­˜swapæ“ä½œ
   - PagedAttentionç®¡ç†KV cache

5. **çŠ¶æ€æ›´æ–°**
   - æ¯æ¬¡stepåæ›´æ–°åºåˆ—çŠ¶æ€
   - æ£€æŸ¥åœæ­¢æ¡ä»¶
   - ç®¡ç†è¯·æ±‚é˜Ÿåˆ—

æ‰€ä»¥ä½ çš„ç†è§£æ˜¯å®Œå…¨æ­£ç¡®çš„ï¼š**stepæ–¹æ³•å°±æ˜¯ç”Ÿæˆnext tokençš„æ ¸å¿ƒï¼Œå¤–éƒ¨é€šè¿‡å¾ªç¯è°ƒç”¨stepæ¥å®Œæˆæ•´ä¸ªæ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹**ï¼

## è°ƒè¯•æŠ€å·§å’Œæœ€ä½³å®è·µ

### ğŸ” å¦‚ä½•è°ƒè¯•step()æ–¹æ³•

```python
# 1. æ·»åŠ è°ƒè¯•æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# 2. æ£€æŸ¥æ¯æ¬¡stepçš„çŠ¶æ€
engine = LLMEngine.from_engine_args(args)
engine.add_request("debug_req", "Hello world", SamplingParams(max_tokens=5))

step_num = 0
while engine.has_unfinished_requests():
    step_num += 1
    print(f"\n=== Step {step_num} ===")
    
    # æ£€æŸ¥è°ƒåº¦å™¨çŠ¶æ€
    print(f"Waiting queue: {len(engine.scheduler[0].waiting)}")
    print(f"Running queue: {len(engine.scheduler[0].running)}")
    print(f"Swapped queue: {len(engine.scheduler[0].swapped)}")
    
    # æ‰§è¡Œstep
    outputs = engine.step()
    
    # æ£€æŸ¥è¾“å‡º
    if outputs:
        for i, output in enumerate(outputs):
            print(f"Request {i}: {output.outputs[0].text}")
            print(f"Finished: {output.finished}")
            print(f"Token count: {len(output.outputs[0].token_ids)}")

# 3. ç›‘æ§GPUå†…å­˜ä½¿ç”¨
import torch
if torch.cuda.is_available():
    print(f"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
```

### âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
# 1. æ‰¹å¤§å°è°ƒä¼˜
engine_args = EngineArgs(
    model="your_model",
    max_num_batched_tokens=8192,  # è°ƒæ•´æ‰¹å¤„ç†tokenæ•°
    max_num_seqs=256,             # è°ƒæ•´æœ€å¤§åºåˆ—æ•°
    gpu_memory_utilization=0.95,  # è°ƒæ•´GPUå†…å­˜ä½¿ç”¨ç‡
)

# 2. ä½¿ç”¨é€‚å½“çš„é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100,
    repetition_penalty=1.1
)

# 3. ç›‘æ§ååé‡
import time
start_time = time.time()
total_tokens = 0

while engine.has_unfinished_requests():
    outputs = engine.step()
    for output in outputs:
        total_tokens += len(output.outputs[0].token_ids)

end_time = time.time()
throughput = total_tokens / (end_time - start_time)
print(f"Throughput: {throughput:.2f} tokens/sec")
```

### ğŸ› å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

1. **OOMé”™è¯¯**
   ```python
   # å‡å°‘æ‰¹å¤§å°æˆ–max_num_seqs
   # é™ä½gpu_memory_utilization
   # ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–é‡åŒ–ç‰ˆæœ¬
   ```

2. **ååé‡ä½**
   ```python
   # å¢åŠ max_num_batched_tokens
   # æ£€æŸ¥GPUåˆ©ç”¨ç‡
   # ä½¿ç”¨æ›´é«˜æ•ˆçš„attention backend
   ```

3. **å»¶è¿Ÿé«˜**
   ```python
   # å‡å°‘max_num_seqs
   # ä½¿ç”¨speculative decoding
   # ä¼˜åŒ–KV cacheé…ç½®
   ```

è¿™ä¸ªè¯¦ç»†çš„æºä»£ç åˆ†æåº”è¯¥èƒ½å¸®åŠ©ä½ å®Œå…¨ç†è§£vLLMå¼•æ“`step()`æ–¹æ³•çš„æ¯ä¸€è¡Œä»£ç ï¼ğŸš€ 