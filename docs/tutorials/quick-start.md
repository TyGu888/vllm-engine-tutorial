# vLLM å¿«é€Ÿå¼€å§‹æŒ‡å—

## ç¯å¢ƒæ­å»º

### 1. ç³»ç»Ÿè¦æ±‚
- **GPU**: NVIDIA GPU with Compute Capability 7.0+ (V100, T4, RTX 20/30/40ç³»åˆ—ç­‰)
- **CUDA**: 11.8 æˆ– 12.1+
- **Python**: 3.8+
- **å†…å­˜**: è‡³å°‘8GB GPUæ˜¾å­˜ (7Bæ¨¡å‹)

### 2. å®‰è£…vLLM

```bash
# æ–¹æ³•1: ä»PyPIå®‰è£… (æ¨è)
pip install vllm

# æ–¹æ³•2: ä»æºç å®‰è£… (å¼€å‘ç”¨)
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

### 3. éªŒè¯å®‰è£…

```python
import vllm
print(f"vLLM version: {vllm.__version__}")
```

## ç¬¬ä¸€ä¸ªç¤ºä¾‹

### 1. ç¦»çº¿æ¨ç† (ç®€å•æ¨¡å¼)

```python
from vllm import LLM, SamplingParams

# åˆ›å»ºLLMå®ä¾‹
llm = LLM(
    model="microsoft/DialoGPT-medium",  # å°æ¨¡å‹ï¼Œé€‚åˆæµ‹è¯•
    gpu_memory_utilization=0.7,        # ä½¿ç”¨70%æ˜¾å­˜
)

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,    # æ§åˆ¶éšæœºæ€§
    top_p=0.95,        # nucleus sampling
    max_tokens=128,    # æœ€å¤§ç”Ÿæˆé•¿åº¦
)

# æ‰¹é‡æ¨ç†
prompts = [
    "Hello, my name is",
    "The capital of France is",
    "The future of AI is",
]

# ç”Ÿæˆæ–‡æœ¬
outputs = llm.generate(prompts, sampling_params)

# è¾“å‡ºç»“æœ
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated: {generated_text!r}")
```

### 2. åœ¨çº¿æœåŠ¡ (APIæ¨¡å¼)

```bash
# å¯åŠ¨OpenAIå…¼å®¹çš„APIæœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/DialoGPT-medium \
    --host 0.0.0.0 \
    --port 8000
```

å®¢æˆ·ç«¯è°ƒç”¨:
```python
from openai import OpenAI

# è¿æ¥åˆ°vLLMæœåŠ¡å™¨
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123"  # å¯ä»¥æ˜¯ä»»æ„å€¼
)

# è°ƒç”¨API
completion = client.completions.create(
    model="microsoft/DialoGPT-medium",
    prompt="San Francisco is a",
    max_tokens=64,
    temperature=0.7
)

print(completion.choices[0].text)
```

## è¿›é˜¶ç¤ºä¾‹

### 1. å¤§æ¨¡å‹æ¨ç†

```python
from vllm import LLM, SamplingParams

# ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ (éœ€è¦æ›´å¤šæ˜¾å­˜)
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=2,      # ä½¿ç”¨2ä¸ªGPU
    gpu_memory_utilization=0.9,  # ä½¿ç”¨90%æ˜¾å­˜
    max_model_len=4096,          # æœ€å¤§åºåˆ—é•¿åº¦
)

# å¯¹è¯æ ¼å¼
prompt = """[INST] You are a helpful assistant. Answer the following question: 
What is the capital of Japan? [/INST]"""

sampling_params = SamplingParams(
    temperature=0.1,  # é™ä½éšæœºæ€§ï¼Œæ›´å‡†ç¡®çš„å›ç­”
    max_tokens=256,
)

output = llm.generate([prompt], sampling_params)[0]
print(output.outputs[0].text)
```

### 2. æµå¼è¾“å‡º

```python
from vllm import LLM, SamplingParams

llm = LLM(model="microsoft/DialoGPT-medium")

sampling_params = SamplingParams(
    temperature=0.8,
    max_tokens=128,
    stream=True  # å¯ç”¨æµå¼è¾“å‡º
)

prompt = "Once upon a time, in a distant galaxy"

# æµå¼ç”Ÿæˆ
for output in llm.generate([prompt], sampling_params):
    for stream_output in output.outputs:
        if stream_output.text:
            print(stream_output.text, end='', flush=True)
```

### 3. è‡ªå®šä¹‰é‡‡æ ·å‚æ•°

```python
from vllm import LLM, SamplingParams

llm = LLM(model="microsoft/DialoGPT-medium")

# åˆ›é€ æ€§å†™ä½œè®¾ç½®
creative_params = SamplingParams(
    temperature=1.2,      # é«˜åˆ›é€ æ€§
    top_p=0.9,
    top_k=50,
    repetition_penalty=1.1,
    max_tokens=200,
)

# å‡†ç¡®é—®ç­”è®¾ç½®
precise_params = SamplingParams(
    temperature=0.1,      # ä½éšæœºæ€§
    top_p=0.95,
    max_tokens=100,
)

prompts = [
    "Write a creative story about a robot:",
    "What is 2+2?",
]

# ä½¿ç”¨ä¸åŒå‚æ•°
creative_output = llm.generate([prompts[0]], creative_params)
precise_output = llm.generate([prompts[1]], precise_params)

print("Creative:", creative_output[0].outputs[0].text)
print("Precise:", precise_output[0].outputs[0].text)
```

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. å†…å­˜ä¼˜åŒ–

```python
# è°ƒæ•´æ˜¾å­˜ä½¿ç”¨ç‡
llm = LLM(
    model="your-model",
    gpu_memory_utilization=0.85,  # æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´
    swap_space=4,                 # CPUäº¤æ¢ç©ºé—´ (GB)
)
```

### 2. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# è¾ƒå¤§çš„æ‰¹å¤„ç†å¯ä»¥æé«˜ååé‡
sampling_params = SamplingParams(max_tokens=100)

# ä¸€æ¬¡å¤„ç†å¤šä¸ªè¯·æ±‚
large_batch = [f"Question {i}: " for i in range(32)]
outputs = llm.generate(large_batch, sampling_params)
```

### 3. åˆ†å¸ƒå¼æ¨ç†

```bash
# å¤šGPUæ¨ç†
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-13b-hf \
    --tensor-parallel-size 4 \
    --pipeline-parallel-size 2
```

## å¸¸è§é—®é¢˜æ’æŸ¥

### 1. æ˜¾å­˜ä¸è¶³ (CUDA OOM)

**è§£å†³æ–¹æ¡ˆ**:
```python
# é™ä½æ˜¾å­˜ä½¿ç”¨ç‡
llm = LLM(model="your-model", gpu_memory_utilization=0.7)

# æˆ–å‡å°‘æœ€å¤§åºåˆ—é•¿åº¦
llm = LLM(model="your-model", max_model_len=2048)
```

### 2. æ¨¡å‹åŠ è½½å¤±è´¥

**æ£€æŸ¥**:
```bash
# éªŒè¯æ¨¡å‹è·¯å¾„
ls /path/to/model/

# æ£€æŸ¥Hugging Face token (ç§æœ‰æ¨¡å‹)
export HF_TOKEN=your_token_here
```

### 3. æ€§èƒ½è¾ƒæ…¢

**ä¼˜åŒ–**:
```python
# å¯ç”¨CUDAå›¾ä¼˜åŒ–
llm = LLM(
    model="your-model", 
    enforce_eager=False,  # å¯ç”¨CUDAå›¾
)

# æˆ–è°ƒæ•´æ‰¹å¤„ç†å¤§å°
sampling_params = SamplingParams(
    max_tokens=64,  # å‡å°‘ç”Ÿæˆé•¿åº¦
)
```

## ä¸‹ä¸€æ­¥

1. ğŸ“– é˜…è¯» [ä¸»æ•™ç¨‹](../README.md) äº†è§£æ¶æ„è¯¦æƒ…
2. ğŸ”§ æŸ¥çœ‹ [å¼€å‘æŒ‡å—](development-guide.md) å­¦ä¹ è´¡çŒ®ä»£ç 
3. ğŸ“Š è¿è¡Œ [æ€§èƒ½æµ‹è¯•](benchmarks.md) è¯„ä¼°æ•ˆæœ
4. ğŸš€ éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä½¿ç”¨

---

**æç¤º**: å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ [vLLMæ–‡æ¡£](https://docs.vllm.ai/) æˆ–åœ¨GitHubæäº¤issueã€‚ 