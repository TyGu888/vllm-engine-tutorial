# vLLM Engine æ¶æ„ä¸å¼€å‘æ•™ç¨‹

## ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ¶æ„å›¾è§£](#æ¶æ„å›¾è§£)
- [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
- [å·¥ä½œæµç¨‹](#å·¥ä½œæµç¨‹)
- [å¼€å‘æŒ‡å—](#å¼€å‘æŒ‡å—)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## æ¦‚è¿°

vLLMæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“ï¼Œé‡‡ç”¨PagedAttentionæŠ€æœ¯å®ç°é«˜æ•ˆçš„å†…å­˜ç®¡ç†å’Œè¯·æ±‚è°ƒåº¦ã€‚æœ¬æ•™ç¨‹å°†å¸®åŠ©ä½ æ·±å…¥ç†è§£vLLMçš„æ¶æ„è®¾è®¡ï¼Œå¹¶æŒæ¡å‚ä¸å¼€å‘çš„å…³é”®çŸ¥è¯†ã€‚

### æ ¸å¿ƒç‰¹æ€§
- ğŸš€ **é«˜ååé‡**: è¿ç»­æ‰¹å¤„ç† + PagedAttention
- ğŸ’¾ **å†…å­˜é«˜æ•ˆ**: åŠ¨æ€å†…å­˜åˆ†é… + KVç¼“å­˜ä¼˜åŒ–  
- ğŸ”„ **æ™ºèƒ½è°ƒåº¦**: å¤šé˜Ÿåˆ—ç®¡ç† + å‰ç¼€ç¼“å­˜
- ğŸŒ **åˆ†å¸ƒå¼**: å¼ é‡å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œ
- ğŸ”Œ **æ˜“é›†æˆ**: OpenAIå…¼å®¹API

## æ¶æ„å›¾è§£

### 1. æ•´ä½“ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    subgraph "Client Layer"
        CLI["CLI Interface"]
        API["OpenAI API"]
        LLM["LLM Class"]
    end
    
    subgraph "Engine Layer"
        AsyncEngine["AsyncLLMEngine<br/>(å¼‚æ­¥å¼•æ“)"]
        SyncEngine["LLMEngine<br/>(åŒæ­¥å¼•æ“)"]
        V1Engine["V1 LLMEngine<br/>(æ–°æ¶æ„)"]
    end
    
    subgraph "Core Components"
        Scheduler["Scheduler<br/>(è°ƒåº¦å™¨)"]
        InputProc["Input Processor<br/>(è¾“å…¥å¤„ç†å™¨)"]
        OutputProc["Output Processor<br/>(è¾“å‡ºå¤„ç†å™¨)"]
        Tokenizer["Tokenizer<br/>(åˆ†è¯å™¨)"]
    end
    
    subgraph "Memory Management"
        BlockMgr["Block Manager<br/>(å†…å­˜å—ç®¡ç†å™¨)"]
        KVCache["KV Cache<br/>(é”®å€¼ç¼“å­˜)"]
        CacheEngine["Cache Engine<br/>(ç¼“å­˜å¼•æ“)"]
    end
    
    subgraph "Model Execution"
        ModelExec["Model Executor<br/>(æ¨¡å‹æ‰§è¡Œå™¨)"]
        Worker["Worker<br/>(å·¥ä½œè¿›ç¨‹)"]
        ModelRunner["Model Runner<br/>(æ¨¡å‹è¿è¡Œå™¨)"]
    end
    
    CLI --> AsyncEngine
    API --> AsyncEngine
    LLM --> SyncEngine
    AsyncEngine --> SyncEngine
    SyncEngine --> Scheduler
    SyncEngine --> InputProc
    SyncEngine --> OutputProc
    Scheduler --> BlockMgr
    BlockMgr --> KVCache
    SyncEngine --> ModelExec
    ModelExec --> Worker
    Worker --> ModelRunner
```

### 2. è¯·æ±‚å¤„ç†æµç¨‹

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant Engine as LLMEngine
    participant Scheduler as è°ƒåº¦å™¨
    participant BlockMgr as å†…å­˜ç®¡ç†å™¨
    participant Executor as æ¨¡å‹æ‰§è¡Œå™¨
    participant Worker as å·¥ä½œè¿›ç¨‹
    
    Client->>Engine: å‘é€æ¨ç†è¯·æ±‚
    Engine->>Engine: è¾“å…¥é¢„å¤„ç†&åˆ†è¯
    Engine->>Scheduler: æ·»åŠ åˆ°ç­‰å¾…é˜Ÿåˆ—
    
    loop è°ƒåº¦å¾ªç¯
        Scheduler->>BlockMgr: æ£€æŸ¥å†…å­˜å¯ç”¨æ€§
        Scheduler->>Scheduler: é€‰æ‹©æ‰§è¡Œè¯·æ±‚
        Scheduler->>Engine: è¿”å›è°ƒåº¦ç»“æœ
    end
    
    Engine->>Executor: æ‰§è¡Œæ¨¡å‹æ¨ç†
    Executor->>Worker: åˆ†å‘åˆ°Worker
    Worker->>Worker: æ¨¡å‹å‰å‘ä¼ æ’­
    Worker->>Executor: è¿”å›è¾“å‡º
    Executor->>Engine: æ±‡æ€»ç»“æœ
    Engine->>Client: è¿”å›å“åº”
```

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. LLMEngine (å¼•æ“æ ¸å¿ƒ)
**ä½ç½®**: `vllm/engine/llm_engine.py`

```python
class LLMEngine:
    """vLLMçš„æ ¸å¿ƒå¼•æ“ï¼Œè´Ÿè´£åè°ƒæ‰€æœ‰ç»„ä»¶"""
    
    def __init__(self, vllm_config, executor_class, ...):
        # åˆå§‹åŒ–åˆ†è¯å™¨
        self.tokenizer = self._init_tokenizer()
        # åˆå§‹åŒ–è°ƒåº¦å™¨
        self.scheduler = Scheduler(...)
        # åˆå§‹åŒ–æ¨¡å‹æ‰§è¡Œå™¨
        self.model_executor = executor_class(...)
        # åˆå§‹åŒ–è¾“å…¥/è¾“å‡ºå¤„ç†å™¨
        self.input_preprocessor = InputPreprocessor(...)
        self.output_processor = SequenceGroupOutputProcessor(...)
    
    def step(self) -> List[RequestOutput]:
        """æ‰§è¡Œä¸€æ¬¡æ¨ç†æ­¥éª¤"""
        # 1. è°ƒåº¦è¯·æ±‚
        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()
        # 2. æ‰§è¡Œæ¨¡å‹
        output = self.model_executor.execute_model(...)
        # 3. å¤„ç†è¾“å‡º
        return self.output_processor.process_outputs(...)
```

**å…³é”®èŒè´£**:
- åè°ƒå„ä¸ªç»„ä»¶å·¥ä½œ
- ç®¡ç†è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ
- æ§åˆ¶æ¨ç†æµç¨‹

#### LLMEngine.step() - æ ¸å¿ƒæ‰§è¡Œæ–¹æ³• ğŸ”¥

`step()` æ˜¯vLLMæœ€å…³é”®çš„æ–¹æ³•ï¼Œæ‰§è¡Œ**è°ƒåº¦-æ‰§è¡Œ-å¤„ç†**çš„å®Œæ•´å‘¨æœŸï¼š

**é‡è¦æ¦‚å¿µæ¾„æ¸…**: `step()` **ä¸æ˜¯**åªdecodeä¸€ä¸ªtokenï¼Œè€Œæ˜¯å¤„ç†æ•´ä¸ªbatchçš„ä¸€æ¬¡å®Œæ•´æ¨ç†è¿­ä»£

**ä¸‰å¤§æ ¸å¿ƒé˜¶æ®µ**:
1. **è°ƒåº¦é˜¶æ®µ**: é€‰æ‹©è¯·æ±‚ã€åˆ†é…å†…å­˜ã€å†³å®šswapæ“ä½œ  
2. **æ‰§è¡Œé˜¶æ®µ**: GPUæ¨ç†è®¡ç®—ï¼ˆattention + FFNï¼‰
3. **å¤„ç†é˜¶æ®µ**: è§£ç æ–‡æœ¬ã€æ›´æ–°çŠ¶æ€ã€æ£€æŸ¥å®Œæˆæ¡ä»¶

```python
def step(self) -> List[RequestOutput]:
    # é˜¶æ®µ1: è°ƒåº¦ - é€‰æ‹©è¦æ‰§è¡Œçš„è¯·æ±‚
    if not self._has_remaining_steps():
        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()
    
    # é˜¶æ®µ2: æ‰§è¡Œ - è¿è¡Œæ¨¡å‹æ¨ç†  
    if not scheduler_outputs.is_empty():
        outputs = self.model_executor.execute_model(execute_model_req)
    
    # é˜¶æ®µ3: å¤„ç† - ç”Ÿæˆæœ€ç»ˆè¾“å‡º
    request_outputs = self._process_model_outputs(outputs)
    return request_outputs
```

**æ¯æ¬¡stepå¯èƒ½åŒ…å«**:
- æ–°è¯·æ±‚çš„prefillï¼ˆå¤„ç†å®Œæ•´promptï¼‰
- ç°æœ‰è¯·æ±‚çš„decodeï¼ˆç”Ÿæˆæ–°tokenï¼‰
- å†…å­˜ç®¡ç†æ“ä½œï¼ˆswap in/outï¼‰
- å¤šä¸ªè¯·æ±‚çš„batchå¤„ç†

è¯¦ç»†è§£æ: 
- [â†’ Engine Stepæ–¹æ³•æ·±åº¦è§£æ](engine-step-analysis.md)
- [â†’ Stepæ–¹æ³•æŠ€æœ¯ç»†èŠ‚æ·±å…¥è§£æ](step-technical-details.md)
- [â†’ **Stepæºä»£ç é€è¡Œè§£æ**](step-source-code-analysis.md) â­
- [â†’ Stepä½¿ç”¨ç¤ºä¾‹ä»£ç ](examples/step_usage_example.py)

### 2. Scheduler (è°ƒåº¦å™¨)
**ä½ç½®**: `vllm/core/scheduler.py`

```python
class Scheduler:
    """æ™ºèƒ½è¯·æ±‚è°ƒåº¦å™¨"""
    
    def __init__(self, scheduler_config, cache_config, ...):
        # ä¸‰ä¸ªé˜Ÿåˆ—ç®¡ç†ä¸åŒçŠ¶æ€çš„è¯·æ±‚
        self.waiting: Deque[SequenceGroup] = deque()  # ç­‰å¾…é˜Ÿåˆ—
        self.running: Deque[SequenceGroup] = deque()  # è¿è¡Œé˜Ÿåˆ—  
        self.swapped: Deque[SequenceGroup] = deque()  # äº¤æ¢é˜Ÿåˆ—
        # å†…å­˜ç®¡ç†
        self.block_manager = BlockSpaceManager(...)
    
    def schedule(self) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs]:
        """æ ¸å¿ƒè°ƒåº¦é€»è¾‘"""
        # 1. è°ƒåº¦è¿è¡Œä¸­çš„è¯·æ±‚ (decode)
        running_scheduled = self._schedule_running(budget, ...)
        # 2. è°ƒåº¦äº¤æ¢å‡ºçš„è¯·æ±‚ (swap in)
        swapped_in = self._schedule_swapped(budget, ...)
        # 3. è°ƒåº¦ç­‰å¾…ä¸­çš„è¯·æ±‚ (prefill)
        prefills = self._schedule_prefills(budget, ...)
        
        return seq_group_metadata_list, scheduler_outputs
```

**å…³é”®æ¦‚å¿µ**:
- **WAITING**: æ–°æ¥çš„è¯·æ±‚ï¼Œç­‰å¾…prefill
- **RUNNING**: æ­£åœ¨ç”Ÿæˆçš„è¯·æ±‚ï¼Œè¿›è¡Œdecode
- **SWAPPED**: å†…å­˜ä¸è¶³æ—¶æš‚å­˜åˆ°CPUçš„è¯·æ±‚

### 3. Block Manager (å†…å­˜ç®¡ç†)
**ä½ç½®**: `vllm/core/block_manager.py`

```python
class SelfAttnBlockSpaceManager:
    """PagedAttentionçš„å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, block_size, num_gpu_blocks, num_cpu_blocks, ...):
        self.block_size = block_size  # é€šå¸¸æ˜¯16
        self.block_allocator = CpuGpuBlockAllocator.create(...)
        self.block_tables: Dict[SeqId, BlockTable] = {}
        
    def allocate(self, seq_group: SequenceGroup) -> None:
        """ä¸ºåºåˆ—ç»„åˆ†é…å†…å­˜å—"""
        for seq in seq_group.get_seqs():
            block_table = self._allocate_sequence(seq)
            self.block_tables[seq.seq_id] = block_table
    
    def can_append_slots(self, seq_group, num_lookahead_slots) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜ç»§ç»­ç”Ÿæˆ"""
        # è®¡ç®—éœ€è¦çš„æ–°å—æ•°é‡
        # æ£€æŸ¥GPUç©ºé—²å—æ•°é‡
        return num_touched_blocks <= num_free_gpu_blocks
```

**PagedAttentionä¼˜åŠ¿**:
- æ¶ˆé™¤å†…å­˜ç¢ç‰‡
- æ”¯æŒåŠ¨æ€åºåˆ—é•¿åº¦
- å®ç°å†…å­˜å…±äº«(prefix caching)

### 4. Model Executor (æ¨¡å‹æ‰§è¡Œ)
**ä½ç½®**: `vllm/executor/`

```python
# å•è¿›ç¨‹æ‰§è¡Œå™¨
class UniProcExecutor(ExecutorBase):
    def execute_model(self, execute_model_req) -> List[SamplerOutput]:
        return self.driver_worker.execute_model(execute_model_req)

# å¤šè¿›ç¨‹æ‰§è¡Œå™¨  
class MultiprocessingDistributedExecutor(DistributedExecutorBase):
    def execute_model(self, execute_model_req) -> List[SamplerOutput]:
        # å¹¶è¡Œæ‰§è¡Œ
        driver_outputs = self._driver_execute_model(execute_model_req)
        return driver_outputs
```

**æ”¯æŒçš„æ‰§è¡Œæ¨¡å¼**:
- **å•è¿›ç¨‹**: å°æ¨¡å‹æˆ–è°ƒè¯•
- **å¤šè¿›ç¨‹**: å¼ é‡å¹¶è¡Œ
- **Ray**: å¤§è§„æ¨¡åˆ†å¸ƒå¼

## å·¥ä½œæµç¨‹

### å…¸å‹çš„æ¨ç†æµç¨‹

1. **è¯·æ±‚æ¥æ”¶**
   ```python
   # ç”¨æˆ·å‘é€è¯·æ±‚
   response = llm.generate("Hello, how are you?", sampling_params)
   ```

2. **è¾“å…¥å¤„ç†**
   ```python
   # åˆ†è¯å’Œé¢„å¤„ç†
   processed_inputs = input_preprocessor.preprocess(prompt, ...)
   seq_group = SequenceGroup(request_id, seqs, ...)
   ```

3. **è°ƒåº¦å†³ç­–**
   ```python
   # è°ƒåº¦å™¨é€‰æ‹©è¦æ‰§è¡Œçš„è¯·æ±‚
   scheduler_outputs = scheduler.schedule()
   # åŒ…å«: prefill requests + decode requests
   ```

4. **å†…å­˜åˆ†é…**
   ```python
   # åˆ†é…KV cacheå†…å­˜å—
   block_manager.allocate(seq_group)
   # æ›´æ–°block tables
   ```

5. **æ¨¡å‹æ‰§è¡Œ**
   ```python
   # æ‰¹é‡æ‰§è¡Œæ¨¡å‹æ¨ç†
   model_outputs = model_executor.execute_model(execute_model_req)
   # åŒ…å«: attentionè®¡ç®— + FFN + sampling
   ```

6. **è¾“å‡ºå¤„ç†**
   ```python
   # å¤„ç†æ¨¡å‹è¾“å‡ºï¼Œæ›´æ–°åºåˆ—
   request_outputs = output_processor.process_outputs(model_outputs)
   ```

### å†…å­˜ç®¡ç†æµç¨‹

```python
# 1. åˆå§‹åŒ–æ—¶ç¡®å®šå¯ç”¨å†…å­˜å—æ•°é‡
num_gpu_blocks, num_cpu_blocks = model_executor.determine_num_available_blocks()

# 2. ä¸ºæ¯ä¸ªåºåˆ—åˆ†é…block table
block_table = []
for token_chunk in sequence_tokens:
    if need_new_block:
        new_block = block_allocator.allocate()
        block_table.append(new_block)

# 3. åœ¨attentionè®¡ç®—ä¸­ä½¿ç”¨blocks
attention_output = paged_attention(query, key_cache, value_cache, block_tables)

# 4. åºåˆ—å®Œæˆåé‡Šæ”¾blocks
block_allocator.free(completed_sequence_blocks)
```

## å¼€å‘æŒ‡å—

### ç¯å¢ƒå‡†å¤‡

```bash
# 1. å…‹éš†ä»£ç 
git clone https://github.com/vllm-project/vllm.git
cd vllm

# 2. å®‰è£…ä¾èµ–
pip install -e .

# 3. è¿è¡Œæµ‹è¯•
pytest tests/ -v
```

### å¸¸è§å¼€å‘ä»»åŠ¡

#### 1. æ·»åŠ æ–°çš„é‡‡æ ·ç®—æ³•

```python
# åœ¨ vllm/sampling_params.py ä¸­æ·»åŠ æ–°å‚æ•°
@dataclass
class SamplingParams:
    my_new_param: float = 1.0

# åœ¨ vllm/model_executor/layers/sampler.py ä¸­å®ç°é€»è¾‘
def _apply_my_new_sampling(logits: torch.Tensor, my_new_param: float):
    # å®ç°æ–°çš„é‡‡æ ·é€»è¾‘
    pass
```

#### 2. ä¼˜åŒ–è°ƒåº¦ç­–ç•¥

```python
# åœ¨ vllm/core/scheduler.py ä¸­ä¿®æ”¹è°ƒåº¦é€»è¾‘
def _schedule_prefills(self, budget, curr_loras, ...):
    # æ·»åŠ æ–°çš„è°ƒåº¦ç­–ç•¥
    # ä¾‹å¦‚: åŸºäºåºåˆ—é•¿åº¦çš„ä¼˜å…ˆçº§
    sorted_waiting = sorted(self.waiting, key=lambda x: len(x.prompt))
    
    for seq_group in sorted_waiting:
        if budget.can_schedule(...):
            # è°ƒåº¦è¿™ä¸ªåºåˆ—ç»„
            pass
```

#### 3. æ·»åŠ æ–°çš„Attentionæœºåˆ¶

```python
# åœ¨ vllm/attention/ ä¸‹åˆ›å»ºæ–°çš„attention backend
class MyAttentionBackend(AttentionBackend):
    def forward(self, query, key, value, ...):
        # å®ç°æ–°çš„attentionç®—æ³•
        pass

# åœ¨ vllm/attention/__init__.py ä¸­æ³¨å†Œ
def get_attn_backend(head_size, dtype, ...):
    if my_condition:
        return MyAttentionBackend()
```

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
```bash
export VLLM_LOGGING_LEVEL=DEBUG
```

#### 2. åˆ†æå†…å­˜ä½¿ç”¨
```python
# åœ¨ä»£ç ä¸­æ·»åŠ å†…å­˜ç›‘æ§
import torch
print(f"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
```

#### 3. æ€§èƒ½åˆ†æ
```python
# ä½¿ç”¨profiler
with torch.profiler.profile() as prof:
    output = model_executor.execute_model(req)
prof.export_chrome_trace("trace.json")
```

### ä»£ç ç»“æ„å¯¼èˆª

```
vllm/
â”œâ”€â”€ engine/              # å¼•æ“æ ¸å¿ƒ
â”‚   â”œâ”€â”€ llm_engine.py   # ä¸»å¼•æ“
â”‚   â””â”€â”€ async_llm_engine.py
â”œâ”€â”€ core/               # æ ¸å¿ƒç®—æ³•
â”‚   â”œâ”€â”€ scheduler.py    # è°ƒåº¦å™¨
â”‚   â””â”€â”€ block_manager.py # å†…å­˜ç®¡ç†
â”œâ”€â”€ executor/           # æ‰§è¡Œå™¨
â”‚   â”œâ”€â”€ executor_base.py
â”‚   â””â”€â”€ ray_distributed_executor.py
â”œâ”€â”€ worker/             # å·¥ä½œè¿›ç¨‹
â”‚   â”œâ”€â”€ worker.py       # ä¸»worker
â”‚   â””â”€â”€ model_runner.py # æ¨¡å‹è¿è¡Œ
â”œâ”€â”€ attention/          # Attentionå®ç°
â”‚   â””â”€â”€ backends/
â”œâ”€â”€ model_executor/     # æ¨¡å‹æ‰§è¡Œ
â”‚   â””â”€â”€ layers/         # æ¨¡å‹å±‚
â””â”€â”€ entrypoints/        # å…¥å£ç‚¹
    â”œâ”€â”€ llm.py         # LLMç±»
    â””â”€â”€ openai/        # OpenAI API
```

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•è°ƒæ•´å†…å­˜ä½¿ç”¨ï¼Ÿ
```python
# é€šè¿‡gpu_memory_utilizationå‚æ•°
llm = LLM(model="meta-llama/Llama-2-7b-hf", 
          gpu_memory_utilization=0.9)  # ä½¿ç”¨90%æ˜¾å­˜
```

### Q2: å¦‚ä½•å¯ç”¨åˆ†å¸ƒå¼æ¨ç†ï¼Ÿ
```bash
# ä½¿ç”¨tensor parallel
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-70b-hf \
    --tensor-parallel-size 4
```

### Q3: å¦‚ä½•è°ƒè¯•è°ƒåº¦é—®é¢˜ï¼Ÿ
```python
# æŸ¥çœ‹è°ƒåº¦å™¨çŠ¶æ€
print(f"Waiting: {len(scheduler.waiting)}")
print(f"Running: {len(scheduler.running)}")
print(f"Swapped: {len(scheduler.swapped)}")
```

### Q4: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹ï¼Ÿ
1. åœ¨`vllm/model_executor/models/`ä¸‹æ·»åŠ æ¨¡å‹å®ç°
2. åœ¨`vllm/model_executor/models/__init__.py`ä¸­æ³¨å†Œ
3. ç¡®ä¿å®ç°å¿…è¦çš„æ¥å£æ–¹æ³•

---

## æ€»ç»“

vLLMé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„å®ç°äº†é«˜æ•ˆçš„LLMæ¨ç†ï¼š

- **åˆ†å±‚è®¾è®¡**: æ¸…æ™°çš„èŒè´£åˆ†ç¦»
- **æ™ºèƒ½è°ƒåº¦**: å¤šé˜Ÿåˆ— + å†…å­˜æ„ŸçŸ¥
- **å†…å­˜ä¼˜åŒ–**: PagedAttention + å‰ç¼€ç¼“å­˜
- **åˆ†å¸ƒå¼**: æ”¯æŒå¤šç§å¹¶è¡Œç­–ç•¥
- **æ˜“æ‰©å±•**: æ¨¡å—åŒ–çš„ç»„ä»¶è®¾è®¡

æŒæ¡è¿™äº›æ ¸å¿ƒæ¦‚å¿µåï¼Œä½ å°±å¯ä»¥å¼€å§‹å‚ä¸vLLMçš„å¼€å‘äº†ï¼å»ºè®®ä»å°çš„åŠŸèƒ½æ”¹è¿›å¼€å§‹ï¼Œé€æ­¥æ·±å…¥æ ¸å¿ƒç»„ä»¶ã€‚ 