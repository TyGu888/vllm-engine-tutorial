#!/usr/bin/env python3
"""
vLLM APIæœåŠ¡å™¨ä½¿ç”¨æ¼”ç¤º
å±•ç¤ºå¦‚ä½•å¯åŠ¨APIæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯è°ƒç”¨
"""

import requests
import json
import time
import asyncio
from openai import OpenAI

def demo_openai_completions():
    """æ¼”ç¤ºOpenAI Completions API"""
    print("=== OpenAI Completions API æ¼”ç¤º ===")
    
    # åˆ›å»ºå®¢æˆ·ç«¯ (éœ€è¦å…ˆå¯åŠ¨vLLMæœåŠ¡å™¨)
    client = OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="token-abc123"  # vLLMä¸éªŒè¯token,å¯ä»¥æ˜¯ä»»æ„å€¼
    )
    
    try:
        # å•ä¸ªcompletionè¯·æ±‚
        completion = client.completions.create(
            model="microsoft/DialoGPT-medium",  # ä½¿ç”¨å¯åŠ¨æœåŠ¡å™¨æ—¶çš„æ¨¡å‹å
            prompt="The future of AI is",
            max_tokens=100,
            temperature=0.8,
            top_p=0.95,
        )
        
        print("å•ä¸ªè¯·æ±‚ç»“æœ:")
        print(f"Generated: {completion.choices[0].text}")
        print()
        
        # å¤šä¸ªprompts
        prompts = [
            "Once upon a time,",
            "In the year 2050,", 
            "The secret to happiness is",
        ]
        
        print("æ‰¹é‡è¯·æ±‚ç»“æœ:")
        for i, prompt in enumerate(prompts):
            completion = client.completions.create(
                model="microsoft/DialoGPT-medium",
                prompt=prompt,
                max_tokens=50,
                temperature=0.7,
            )
            print(f"{i+1}. Prompt: {prompt}")
            print(f"   Generated: {completion.choices[0].text.strip()}")
        print()
            
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿vLLMæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ:")
        print("python -m vllm.entrypoints.openai.api_server --model microsoft/DialoGPT-medium --port 8000")

def demo_openai_chat():
    """æ¼”ç¤ºOpenAI Chat API"""
    print("=== OpenAI Chat API æ¼”ç¤º ===")
    
    client = OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="token-abc123"
    )
    
    try:
        # èŠå¤©å¯¹è¯
        response = client.chat.completions.create(
            model="microsoft/DialoGPT-medium",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Hello! How are you?"},
            ],
            max_tokens=100,
            temperature=0.7,
        )
        
        print("èŠå¤©å¯¹è¯:")
        print(f"User: Hello! How are you?")
        print(f"Assistant: {response.choices[0].message.content}")
        print()
        
        # å¤šè½®å¯¹è¯
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is machine learning?"},
        ]
        
        response = client.chat.completions.create(
            model="microsoft/DialoGPT-medium",
            messages=messages,
            max_tokens=150,
            temperature=0.5,
        )
        
        print("å¤šè½®å¯¹è¯:")
        print(f"User: What is machine learning?")
        print(f"Assistant: {response.choices[0].message.content}")
        
    except Exception as e:
        print(f"èŠå¤©è¯·æ±‚å¤±è´¥: {e}")

def demo_streaming():
    """æ¼”ç¤ºæµå¼è¾“å‡º"""
    print("=== æµå¼è¾“å‡ºæ¼”ç¤º ===")
    
    client = OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="token-abc123"
    )
    
    try:
        print("æµå¼ç”Ÿæˆ (é€å­—è¾“å‡º):")
        print("Prompt: Write a short story about space exploration")
        print("Generated: ", end="", flush=True)
        
        stream = client.completions.create(
            model="microsoft/DialoGPT-medium",
            prompt="Write a short story about space exploration",
            max_tokens=200,
            temperature=0.8,
            stream=True,  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        for chunk in stream:
            if chunk.choices[0].text:
                print(chunk.choices[0].text, end="", flush=True)
        
        print("\n")
        
    except Exception as e:
        print(f"æµå¼è¯·æ±‚å¤±è´¥: {e}")

def demo_raw_http():
    """æ¼”ç¤ºåŸå§‹HTTPè¯·æ±‚"""
    print("=== åŸå§‹HTTPè¯·æ±‚æ¼”ç¤º ===")
    
    url = "http://localhost:8000/v1/completions"
    
    payload = {
        "model": "microsoft/DialoGPT-medium",
        "prompt": "The key to success is",
        "max_tokens": 80,
        "temperature": 0.7,
        "top_p": 0.95,
    }
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer token-abc123"
    }
    
    try:
        response = requests.post(url, json=payload, headers=headers)
        
        if response.status_code == 200:
            result = response.json()
            print("åŸå§‹HTTPè¯·æ±‚æˆåŠŸ:")
            print(f"Prompt: {payload['prompt']}")
            print(f"Generated: {result['choices'][0]['text']}")
            print(f"Status: {response.status_code}")
        else:
            print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            
    except Exception as e:
        print(f"HTTPè¯·æ±‚å¤±è´¥: {e}")
    
    print()

def demo_performance_test():
    """æ¼”ç¤ºæ€§èƒ½æµ‹è¯•"""
    print("=== æ€§èƒ½æµ‹è¯•æ¼”ç¤º ===")
    
    client = OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="token-abc123"
    )
    
    # å¹¶å‘è¯·æ±‚æµ‹è¯•
    async def async_request(prompt, request_id):
        """å¼‚æ­¥è¯·æ±‚å‡½æ•°"""
        try:
            # æ³¨æ„: OpenAIå®¢æˆ·ç«¯ä¸ç›´æ¥æ”¯æŒasync,è¿™é‡Œæ¼”ç¤ºæ¦‚å¿µ
            # å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦ä½¿ç”¨aiohttpç­‰åº“
            completion = client.completions.create(
                model="microsoft/DialoGPT-medium",
                prompt=f"{prompt} (request {request_id})",
                max_tokens=50,
                temperature=0.7,
            )
            return completion.choices[0].text
        except Exception as e:
            return f"Error: {e}"
    
    try:
        # é¡ºåºè¯·æ±‚æ€§èƒ½æµ‹è¯•
        print("é¡ºåºè¯·æ±‚æ€§èƒ½æµ‹è¯•:")
        prompts = [f"Question {i}: What is" for i in range(5)]
        
        start_time = time.time()
        results = []
        for prompt in prompts:
            completion = client.completions.create(
                model="microsoft/DialoGPT-medium",
                prompt=prompt,
                max_tokens=30,
                temperature=0.7,
            )
            results.append(completion.choices[0].text)
        
        sequential_time = time.time() - start_time
        print(f"å¤„ç† {len(prompts)} ä¸ªè¯·æ±‚è€—æ—¶: {sequential_time:.2f}ç§’")
        print(f"å¹³å‡æ¯ä¸ªè¯·æ±‚: {sequential_time/len(prompts):.2f}ç§’")
        print()
        
    except Exception as e:
        print(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

def demo_model_info():
    """æ¼”ç¤ºè·å–æ¨¡å‹ä¿¡æ¯"""
    print("=== æ¨¡å‹ä¿¡æ¯æ¼”ç¤º ===")
    
    try:
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        response = requests.get("http://localhost:8000/v1/models")
        
        if response.status_code == 200:
            models = response.json()
            print("å¯ç”¨æ¨¡å‹:")
            for model in models.get('data', []):
                print(f"- {model['id']}")
        else:
            print(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code}")
        
        print()
        
    except Exception as e:
        print(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")

def print_server_instructions():
    """æ‰“å°å¯åŠ¨æœåŠ¡å™¨çš„è¯´æ˜"""
    print("=" * 60)
    print("vLLM APIæœåŠ¡å™¨æ¼”ç¤º")
    print("=" * 60)
    print()
    print("åœ¨è¿è¡Œæ­¤æ¼”ç¤ºä¹‹å‰ï¼Œè¯·å…ˆå¯åŠ¨vLLMæœåŠ¡å™¨:")
    print()
    print("åŸºæœ¬å¯åŠ¨å‘½ä»¤:")
    print("python -m vllm.entrypoints.openai.api_server \\")
    print("    --model microsoft/DialoGPT-medium \\")
    print("    --host 0.0.0.0 \\")
    print("    --port 8000")
    print()
    print("å¸¦æ›´å¤šå‚æ•°çš„å¯åŠ¨å‘½ä»¤:")
    print("python -m vllm.entrypoints.openai.api_server \\")
    print("    --model microsoft/DialoGPT-medium \\")
    print("    --host 0.0.0.0 \\")
    print("    --port 8000 \\")
    print("    --gpu-memory-utilization 0.8 \\")
    print("    --max-model-len 1024")
    print()
    print("æœåŠ¡å™¨å¯åŠ¨åï¼Œè®¿é—® http://localhost:8000/docs æŸ¥çœ‹APIæ–‡æ¡£")
    print("=" * 60)
    print()

def check_server_health():
    """æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code == 200:
            print("âœ… vLLMæœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
            return True
        else:
            print(f"âŒ æœåŠ¡å™¨å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except requests.exceptions.RequestException:
        print("âŒ æ— æ³•è¿æ¥åˆ°vLLMæœåŠ¡å™¨ (http://localhost:8000)")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print_server_instructions()
    
    # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
    if not check_server_health():
        print("\nè¯·å…ˆæŒ‰ç…§ä¸Šè¿°è¯´æ˜å¯åŠ¨vLLMæœåŠ¡å™¨ï¼Œç„¶åé‡æ–°è¿è¡Œæ­¤æ¼”ç¤ºã€‚")
        return
    
    print()
    
    try:
        # è¿è¡Œå„ç§æ¼”ç¤º
        demo_model_info()
        demo_openai_completions()
        demo_openai_chat()
        demo_streaming()
        demo_raw_http()
        demo_performance_test()
        
        print("ğŸ‰ æ‰€æœ‰APIæ¼”ç¤ºå®Œæˆ!")
        print()
        print("è¿›ä¸€æ­¥æ¢ç´¢:")
        print("1. æŸ¥çœ‹APIæ–‡æ¡£: http://localhost:8000/docs")
        print("2. å°è¯•ä¸åŒçš„æ¨¡å‹å‚æ•°")
        print("3. æµ‹è¯•æ›´å¤§çš„æ¨¡å‹å’Œæ‰¹æ¬¡")
        
    except KeyboardInterrupt:
        print("\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\næ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")

if __name__ == "__main__":
    main() 