#!/usr/bin/env python3
"""
Scheduler å·¥ä½œæœºåˆ¶ç¤ºä¾‹

æœ¬ç¤ºä¾‹æ¨¡æ‹ŸvLLM Schedulerçš„æ ¸å¿ƒå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- è¯·æ±‚é˜Ÿåˆ—ç®¡ç† (waiting, running, swapped)
- KV Cacheå†…å­˜åˆ†é…å’Œå›æ”¶
- æŠ¢å å’Œswapæœºåˆ¶
- Prefix cachingä¼˜åŒ–
"""

import time
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from enum import Enum


class RequestStatus(Enum):
    WAITING = "waiting"
    RUNNING = "running" 
    SWAPPED = "swapped"
    FINISHED = "finished"


class AllocStatus(Enum):
    OK = "ok"
    LATER = "later"
    NEVER = "never"


@dataclass
class Request:
    """æ¨¡æ‹Ÿè¯·æ±‚"""
    request_id: str
    prompt_tokens: List[int]
    status: RequestStatus = RequestStatus.WAITING
    allocated_blocks: List[int] = None
    generated_tokens: int = 0
    priority: int = 0
    
    def __post_init__(self):
        if self.allocated_blocks is None:
            self.allocated_blocks = []


@dataclass
class SchedulingBudget:
    """è°ƒåº¦é¢„ç®—"""
    token_budget: int
    max_num_seqs: int
    current_tokens: int = 0
    current_seqs: int = 0
    
    def can_schedule(self, num_tokens: int, num_seqs: int) -> bool:
        return (self.current_tokens + num_tokens <= self.token_budget and
                self.current_seqs + num_seqs <= self.max_num_seqs)
    
    def add_request(self, num_tokens: int, num_seqs: int = 1):
        self.current_tokens += num_tokens
        self.current_seqs += num_seqs


class BlockManager:
    """æ¨¡æ‹ŸBlock Manager"""
    
    def __init__(self, block_size: int = 16, num_gpu_blocks: int = 100, 
                 num_cpu_blocks: int = 200):
        self.block_size = block_size
        self.num_gpu_blocks = num_gpu_blocks
        self.num_cpu_blocks = num_cpu_blocks
        
        self.free_gpu_blocks = list(range(num_gpu_blocks))
        self.free_cpu_blocks = list(range(num_cpu_blocks))
        self.allocated_gpu_blocks = set()
        self.allocated_cpu_blocks = set()
        
        # æ¨¡æ‹Ÿblockæ˜ å°„è¡¨
        self.block_tables: Dict[str, List[int]] = {}
        self.swapped_blocks: Dict[str, List[int]] = {}
        
        # Prefix cache
        self.prefix_cache: Dict[str, List[int]] = {}
        
    def get_required_blocks(self, num_tokens: int) -> int:
        """è®¡ç®—æ‰€éœ€çš„blockæ•°é‡"""
        return (num_tokens + self.block_size - 1) // self.block_size
    
    def can_allocate(self, request: Request) -> AllocStatus:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ†é…å†…å­˜"""
        required_blocks = self.get_required_blocks(len(request.prompt_tokens))
        
        # æ£€æŸ¥prefix cache
        prefix_key = str(request.prompt_tokens[:10])  # ç®€åŒ–çš„prefix
        if prefix_key in self.prefix_cache:
            cached_blocks = len(self.prefix_cache[prefix_key])
            required_blocks = max(0, required_blocks - cached_blocks)
            print(f"ğŸ¯ Request {request.request_id}: Prefix cache hit! "
                  f"Saved {cached_blocks} blocks")
        
        if len(self.free_gpu_blocks) >= required_blocks:
            return AllocStatus.OK
        elif required_blocks <= self.num_gpu_blocks:
            return AllocStatus.LATER
        else:
            return AllocStatus.NEVER
    
    def allocate(self, request: Request) -> bool:
        """åˆ†é…å†…å­˜"""
        required_blocks = self.get_required_blocks(len(request.prompt_tokens))
        
        # æ£€æŸ¥prefix cache
        prefix_key = str(request.prompt_tokens[:10])
        cached_blocks = []
        if prefix_key in self.prefix_cache:
            cached_blocks = self.prefix_cache[prefix_key].copy()
            required_blocks = max(0, required_blocks - len(cached_blocks))
        
        if len(self.free_gpu_blocks) < required_blocks:
            return False
        
        # åˆ†é…æ–°blocks
        new_blocks = []
        for _ in range(required_blocks):
            if self.free_gpu_blocks:
                block_id = self.free_gpu_blocks.pop(0)
                new_blocks.append(block_id)
                self.allocated_gpu_blocks.add(block_id)
        
        # ç»„åˆcached blockså’Œæ–°blocks
        all_blocks = cached_blocks + new_blocks
        self.block_tables[request.request_id] = all_blocks
        request.allocated_blocks = all_blocks
        
        # æ›´æ–°prefix cache
        if not cached_blocks and len(request.prompt_tokens) >= 10:
            self.prefix_cache[prefix_key] = new_blocks[:len(new_blocks)//2]
        
        print(f"ğŸ“¦ Allocated {len(all_blocks)} blocks for {request.request_id} "
              f"(cached: {len(cached_blocks)}, new: {len(new_blocks)})")
        return True
    
    def can_append(self, request: Request) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­ç”Ÿæˆ"""
        # ç®€å•æ£€æŸ¥ï¼šæ¯ä¸ªæ–°tokenå¯èƒ½éœ€è¦æ–°block
        return len(self.free_gpu_blocks) > 0
    
    def append_slot(self, request: Request):
        """ä¸ºæ–°tokenåˆ†é…slot"""
        if not self.can_append(request):
            return False
        
        # å¯èƒ½éœ€è¦æ–°block
        if request.generated_tokens % self.block_size == 0:
            if self.free_gpu_blocks:
                new_block = self.free_gpu_blocks.pop(0)
                self.block_tables[request.request_id].append(new_block)
                self.allocated_gpu_blocks.add(new_block)
                print(f"ğŸ“¦ Added new block {new_block} for {request.request_id}")
        
        request.generated_tokens += 1
        return True
    
    def swap_out(self, request: Request) -> bool:
        """Swap outåˆ°CPU"""
        if request.request_id not in self.block_tables:
            return False
        
        gpu_blocks = self.block_tables[request.request_id]
        
        # æ£€æŸ¥CPUç©ºé—´
        if len(self.free_cpu_blocks) < len(gpu_blocks):
            return False
        
        # åˆ†é…CPU blocks
        cpu_blocks = []
        for _ in range(len(gpu_blocks)):
            cpu_block = self.free_cpu_blocks.pop(0)
            cpu_blocks.append(cpu_block)
            self.allocated_cpu_blocks.add(cpu_block)
        
        # é‡Šæ”¾GPU blocks
        for block in gpu_blocks:
            self.allocated_gpu_blocks.remove(block)
            self.free_gpu_blocks.append(block)
        
        # æ›´æ–°æ˜ å°„
        self.swapped_blocks[request.request_id] = cpu_blocks
        del self.block_tables[request.request_id]
        
        print(f"ğŸ’¾ Swapped out {request.request_id}: "
              f"GPU blocks {gpu_blocks} â†’ CPU blocks {cpu_blocks}")
        return True
    
    def swap_in(self, request: Request) -> bool:
        """Swap inåˆ°GPU"""
        if request.request_id not in self.swapped_blocks:
            return False
        
        cpu_blocks = self.swapped_blocks[request.request_id]
        
        # æ£€æŸ¥GPUç©ºé—´
        if len(self.free_gpu_blocks) < len(cpu_blocks):
            return False
        
        # åˆ†é…GPU blocks
        gpu_blocks = []
        for _ in range(len(cpu_blocks)):
            gpu_block = self.free_gpu_blocks.pop(0)
            gpu_blocks.append(gpu_block)
            self.allocated_gpu_blocks.add(gpu_block)
        
        # é‡Šæ”¾CPU blocks
        for block in cpu_blocks:
            self.allocated_cpu_blocks.remove(block)
            self.free_cpu_blocks.append(block)
        
        # æ›´æ–°æ˜ å°„
        self.block_tables[request.request_id] = gpu_blocks
        del self.swapped_blocks[request.request_id]
        
        print(f"ğŸ“¤ Swapped in {request.request_id}: "
              f"CPU blocks {cpu_blocks} â†’ GPU blocks {gpu_blocks}")
        return True
    
    def free(self, request: Request):
        """é‡Šæ”¾å†…å­˜"""
        if request.request_id in self.block_tables:
            blocks = self.block_tables[request.request_id]
            for block in blocks:
                self.allocated_gpu_blocks.remove(block)
                self.free_gpu_blocks.append(block)
            del self.block_tables[request.request_id]
            print(f"ğŸ—‘ï¸  Freed {len(blocks)} GPU blocks for {request.request_id}")
        
        if request.request_id in self.swapped_blocks:
            blocks = self.swapped_blocks[request.request_id]
            for block in blocks:
                self.allocated_cpu_blocks.remove(block)
                self.free_cpu_blocks.append(block)
            del self.swapped_blocks[request.request_id]
            print(f"ğŸ—‘ï¸  Freed {len(blocks)} CPU blocks for {request.request_id}")
    
    def get_memory_stats(self) -> Dict:
        """è·å–å†…å­˜ç»Ÿè®¡"""
        return {
            "free_gpu_blocks": len(self.free_gpu_blocks),
            "allocated_gpu_blocks": len(self.allocated_gpu_blocks),
            "free_cpu_blocks": len(self.free_cpu_blocks),
            "allocated_cpu_blocks": len(self.allocated_cpu_blocks),
            "gpu_usage": len(self.allocated_gpu_blocks) / self.num_gpu_blocks,
            "cpu_usage": len(self.allocated_cpu_blocks) / self.num_cpu_blocks,
        }


class SimpleScheduler:
    """ç®€åŒ–çš„Schedulerå®ç°"""
    
    def __init__(self, block_manager: BlockManager):
        self.block_manager = block_manager
        
        # ä¸‰ä¸ªé˜Ÿåˆ—
        self.waiting: List[Request] = []
        self.running: List[Request] = []
        self.swapped: List[Request] = []
        
        # è°ƒåº¦é…ç½®
        self.max_num_batched_tokens = 512
        self.max_num_seqs = 8
        
    def add_request(self, request: Request):
        """æ·»åŠ æ–°è¯·æ±‚"""
        request.status = RequestStatus.WAITING
        self.waiting.append(request)
        print(f"â• Added request {request.request_id} to waiting queue")
    
    def schedule(self) -> Dict:
        """ä¸»è°ƒåº¦é€»è¾‘"""
        print("\n" + "="*60)
        print("ğŸ”„ Scheduler å¼€å§‹è°ƒåº¦")
        
        budget = SchedulingBudget(
            token_budget=self.max_num_batched_tokens,
            max_num_seqs=self.max_num_seqs
        )
        
        scheduled_requests = []
        blocks_to_swap_in = []
        blocks_to_swap_out = []
        preempted_requests = []
        
        # 1. è°ƒåº¦runningé˜Ÿåˆ—
        print("\n1ï¸âƒ£ è°ƒåº¦Runningé˜Ÿåˆ—")
        running_to_remove = []
        for request in self.running:
            if request.generated_tokens >= 20:  # æ¨¡æ‹Ÿå®Œæˆæ¡ä»¶
                request.status = RequestStatus.FINISHED
                running_to_remove.append(request)
                self.block_manager.free(request)
                print(f"âœ… Request {request.request_id} finished")
                continue
            
            if not self.block_manager.can_append(request):
                # éœ€è¦æŠ¢å 
                print(f"âš ï¸  Request {request.request_id} éœ€è¦æŠ¢å ")
                if self.block_manager.swap_out(request):
                    request.status = RequestStatus.SWAPPED
                    self.swapped.append(request)
                    running_to_remove.append(request)
                    preempted_requests.append(request.request_id)
                    print(f"ğŸ’¾ Request {request.request_id} è¢«swap out")
                continue
            
            if budget.can_schedule(1, 0):  # decodeéœ€è¦1ä¸ªtoken
                self.block_manager.append_slot(request)
                scheduled_requests.append(request)
                budget.add_request(1, 0)
                print(f"ğŸ¯ Request {request.request_id} ç»§ç»­decode "
                      f"(generated: {request.generated_tokens})")
        
        # ç§»é™¤å·²å¤„ç†çš„è¯·æ±‚
        for request in running_to_remove:
            self.running.remove(request)
        
        # 2. è°ƒåº¦swappedé˜Ÿåˆ—
        print("\n2ï¸âƒ£ è°ƒåº¦Swappedé˜Ÿåˆ—") 
        swapped_to_remove = []
        for request in self.swapped:
            if budget.can_schedule(1, 1):
                if self.block_manager.swap_in(request):
                    request.status = RequestStatus.RUNNING
                    self.running.append(request)
                    swapped_to_remove.append(request)
                    scheduled_requests.append(request)
                    budget.add_request(1, 1)
                    blocks_to_swap_in.append(request.request_id)
                    print(f"ğŸ“¤ Request {request.request_id} è¢«swap in")
        
        for request in swapped_to_remove:
            self.swapped.remove(request)
        
        # 3. è°ƒåº¦waitingé˜Ÿåˆ—
        print("\n3ï¸âƒ£ è°ƒåº¦Waitingé˜Ÿåˆ—")
        waiting_to_remove = []
        for request in self.waiting:
            prompt_tokens = len(request.prompt_tokens)
            
            if not budget.can_schedule(prompt_tokens, 1):
                print(f"â¸ï¸  Request {request.request_id} è¶…å‡ºé¢„ç®—ï¼Œç¨åè°ƒåº¦")
                continue
            
            alloc_status = self.block_manager.can_allocate(request)
            if alloc_status == AllocStatus.OK:
                if self.block_manager.allocate(request):
                    request.status = RequestStatus.RUNNING
                    self.running.append(request)
                    waiting_to_remove.append(request)
                    scheduled_requests.append(request)
                    budget.add_request(prompt_tokens, 1)
                    print(f"ğŸš€ Request {request.request_id} å¼€å§‹prefill")
            elif alloc_status == AllocStatus.LATER:
                print(f"â³ Request {request.request_id} å†…å­˜ä¸è¶³ï¼Œç¨åé‡è¯•")
            else:
                print(f"âŒ Request {request.request_id} æ°¸è¿œæ— æ³•åˆ†é…")
        
        for request in waiting_to_remove:
            self.waiting.remove(request)
        
        # æ„å»ºè°ƒåº¦ç»“æœ
        result = {
            "scheduled_requests": [r.request_id for r in scheduled_requests],
            "num_scheduled_tokens": budget.current_tokens,
            "blocks_to_swap_in": blocks_to_swap_in,
            "blocks_to_swap_out": blocks_to_swap_out,
            "preempted_requests": preempted_requests,
            "queue_lengths": {
                "waiting": len(self.waiting),
                "running": len(self.running),
                "swapped": len(self.swapped)
            }
        }
        
        print(f"\nğŸ“Š è°ƒåº¦ç»“æœ: {result['num_scheduled_tokens']} tokens, "
              f"{len(scheduled_requests)} requests")
        print(f"ğŸ“‹ é˜Ÿåˆ—çŠ¶æ€: Waiting({len(self.waiting)}), "
              f"Running({len(self.running)}), Swapped({len(self.swapped)})")
        
        return result
    
    def get_status(self) -> Dict:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        memory_stats = self.block_manager.get_memory_stats()
        return {
            "queue_lengths": {
                "waiting": len(self.waiting),
                "running": len(self.running),
                "swapped": len(self.swapped)
            },
            "memory_stats": memory_stats
        }


def main():
    """ä¸»æ¼”ç¤ºç¨‹åº"""
    print("ğŸ­ vLLM Scheduler å·¥ä½œæœºåˆ¶æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºç»„ä»¶
    block_manager = BlockManager(
        block_size=16,
        num_gpu_blocks=20,  # æ•…æ„è®¾ç½®å°ä¸€ç‚¹ä»¥æ¼”ç¤ºæŠ¢å 
        num_cpu_blocks=40
    )
    
    scheduler = SimpleScheduler(block_manager)
    
    # åˆ›å»ºæµ‹è¯•è¯·æ±‚
    requests = [
        Request("req_1", list(range(50)), priority=1),   # é•¿è¯·æ±‚
        Request("req_2", list(range(30)), priority=2),   # ä¸­ç­‰è¯·æ±‚
        Request("req_3", list(range(20)), priority=3),   # çŸ­è¯·æ±‚
        Request("req_4", list(range(60)), priority=1),   # é•¿è¯·æ±‚
        Request("req_5", list(range(10)), priority=4),   # å¾ˆçŸ­è¯·æ±‚
        Request("req_6", list(range(10)), priority=4),   # ç›¸åŒprefixï¼Œæµ‹è¯•ç¼“å­˜
    ]
    
    # é€ä¸ªæ·»åŠ è¯·æ±‚
    for i, request in enumerate(requests):
        scheduler.add_request(request)
        
        # æ¯æ·»åŠ ä¸€ä¸ªè¯·æ±‚å°±è°ƒåº¦ä¸€æ¬¡
        print(f"\nğŸ”„ ç¬¬{i+1}è½®è°ƒåº¦")
        schedule_result = scheduler.schedule()
        
        # æ˜¾ç¤ºå†…å­˜çŠ¶æ€
        status = scheduler.get_status()
        memory = status["memory_stats"]
        print(f"ğŸ’¾ GPUä½¿ç”¨ç‡: {memory['gpu_usage']:.1%}, "
              f"ç©ºé—²blocks: {memory['free_gpu_blocks']}")
        
        time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
    
    # ç»§ç»­è°ƒåº¦ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ
    print("\n" + "="*60)
    print("ğŸ”„ ç»§ç»­è°ƒåº¦ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ...")
    
    for step in range(10):
        if (len(scheduler.waiting) == 0 and 
            len(scheduler.running) == 0 and 
            len(scheduler.swapped) == 0):
            print("âœ… æ‰€æœ‰è¯·æ±‚å·²å®Œæˆ!")
            break
        
        print(f"\nğŸ”„ ç¬¬{step+7}è½®è°ƒåº¦")
        schedule_result = scheduler.schedule()
        
        status = scheduler.get_status()
        memory = status["memory_stats"]
        print(f"ğŸ’¾ GPUä½¿ç”¨ç‡: {memory['gpu_usage']:.1%}")
        
        time.sleep(0.3)
    
    # æœ€ç»ˆç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡")
    final_memory = block_manager.get_memory_stats()
    print(f"GPU blocks: {final_memory['free_gpu_blocks']}/{block_manager.num_gpu_blocks} ç©ºé—²")
    print(f"CPU blocks: {final_memory['free_cpu_blocks']}/{block_manager.num_cpu_blocks} ç©ºé—²")
    print(f"Prefix cache å¤§å°: {len(block_manager.prefix_cache)}")
    
    print("\nğŸ¯ æ¼”ç¤ºè¦ç‚¹:")
    print("1. âœ… è¯·æ±‚é˜Ÿåˆ—ç®¡ç†: waiting â†’ running â†’ finished")
    print("2. âœ… å†…å­˜åˆ†é…: åŠ¨æ€åˆ†é…GPU blocksç»™æ–°è¯·æ±‚")
    print("3. âœ… æŠ¢å æœºåˆ¶: GPUå†…å­˜ä¸è¶³æ—¶swap outåˆ°CPU")
    print("4. âœ… Swapæ¢å¤: GPUç©ºé—´é‡Šæ”¾åswap inå›æ¥")
    print("5. âœ… Prefixç¼“å­˜: ç›¸åŒå‰ç¼€çš„è¯·æ±‚å…±äº«blocks")
    print("6. âœ… èµ„æºé¢„ç®—: é™åˆ¶æ¯è½®è°ƒåº¦çš„tokenæ•°å’Œè¯·æ±‚æ•°")


if __name__ == "__main__":
    main() 